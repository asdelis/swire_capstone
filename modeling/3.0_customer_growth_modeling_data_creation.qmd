---
title: "swire data transformation"
format: html
editor: visual
---

```{r setup, include=FALSE}
library(rmarkdown); library(tidyverse); library(knitr); library(matrixStats); library(ROSE); library(DMwR2); library(randomForest); library(C50); library(clusterSim); library(fpc); library(fossil); library(cvTools); library(cluster); library(flexclust); library(mclust); library(viridis); library(ggplot2); library(rpart); library(RWeka); library(kernlab); library(MLmetrics); library(smotefamily); library(caret); library(rminer); library(tictoc); library(rpart.plot); library(psych); library(ROCR); library(glmnet); library(car); library(reshape2); library(ggcorrplot); library(dplyr); library(tidyr); library(grf); library(kableExtra); library(formattable); library(rattle); library(factoextra); library(corrplot); library(dataPreparation); library(scales); library(DT); library(lubridate); library(data.table); library(glue); library(readxl); library(janitor); library(RColorBrewer); library(tidycensus); library(sf); library(doParallel); library(vcd); library(scales); library(nnet)

one_seed<-500

# Suppress warnings globally
options(warn = -1)

# Reference Date
reference_date <- as.Date("2025-01-01")

mydir <- getwd()
setwd(mydir)
```

## 1 Data Transformartion codes

```{r}
# Load the profile CSV
profile_data <- read.csv(file = "customer_profile.csv", 
                         sep = ",", 
                         stringsAsFactors = FALSE)

# Load the address CSV
customer_address <- read.csv(file = "customer_address_and_zip_mapping.csv", 
                             sep = ",", 
                             stringsAsFactors = FALSE)

# Load the transactional CSV
op_data <- read.csv(file = "transactional_data.csv", 
                           sep = ",", 
                           stringsAsFactors = FALSE)
```


**Profile Dataset - Cleaning and Adjustments**
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}

# 1. Customer Profile Dataset cleaning and basic transformations

# Data Structure
str(profile_data)

# Check the number of unique CUSTOMER_NUMBER in profile_data and op_data
print(length(unique(profile_data$CUSTOMER_NUMBER)))
print(length(unique(op_data$CUSTOMER_NUMBER)))

# Convert the date columns to Date format
profile_data$FIRST_DELIVERY_DATE <- as.Date(profile_data$FIRST_DELIVERY_DATE, format = "%m/%d/%Y")
profile_data$ON_BOARDING_DATE <- as.Date(profile_data$ON_BOARDING_DATE, format = "%m/%d/%Y")

# Convert logical to integer for LOCAL_MARKET_PARTNER
profile_data$LOCAL_MARKET_PARTNER <- as.integer(profile_data$LOCAL_MARKET_PARTNER)

# Convert logical to integer for CO2_CUSTOMER
profile_data$CO2_CUSTOMER <- as.integer(profile_data$CO2_CUSTOMER)

# Convert all character columns to factors
profile_data[sapply(profile_data, is.character)] <- lapply(profile_data[sapply(profile_data, is.character)], as.factor)

# Summary
summary(profile_data)

# Remove parentheses, any "-" or "&", then trim extra spaces
profile_data$COLD_DRINK_CHANNEL <- as.factor(gsub("\\s+", " ", gsub("[(]|[)]|[-]|[&]", " ", as.character(profile_data$COLD_DRINK_CHANNEL))))
profile_data$TRADE_CHANNEL <- as.factor(gsub("\\s+", " ", gsub("[(]|[)]|[-]|[&]", " ", as.character(profile_data$TRADE_CHANNEL))))
profile_data$SUB_TRADE_CHANNEL <- as.factor(gsub("\\s+", " ", gsub("[(]|[)]|[-]|[&]", " ", as.character(profile_data$SUB_TRADE_CHANNEL))))


# Verify the changes in the summary
summary(profile_data)

# Display the count of NA values for each variable
cat("NA count for each variable:\n")
sapply(profile_data, function(x) sum(is.na(x)))

# Display the count of NULL values for each variable
cat("NULL count for each variable:\n")
sapply(profile_data, function(x) sum(is.null(x)))

# Replace NA values in PRIMARY_GROUP_NUMBER with zero
profile_data$PRIMARY_GROUP_NUMBER[is.na(profile_data$PRIMARY_GROUP_NUMBER)] <- 0

# Display the count of duplicate 'CUSTOMER_NUMBER' values
cat("Number of duplicate CUSTOMER_NUMBER values:", sum(duplicated(profile_data$CUSTOMER_NUMBER)), "\n")

# Create CHAIN_MEMBER column: 0 for non-members, 1 for members
profile_data$CHAIN_MEMBER <- as.integer(profile_data$PRIMARY_GROUP_NUMBER != 0)
```

- The number of unique CUSTOMER_NUMBER in profile_data is greater than in transactional data. This will be addressed     later before merging the datasets.
- There are no duplicates or missing values for CUSTOMER_NUMBER.
- Date variables were adjusted to the proper format.  
- Logical variables were converted to integers, where 0 represents false and 1 represents true.
- Special characters and extra spaces were removed in factor variables.
- Missing values in the PRIMARY_GROUP_NUMBER field were replaced with zero.  
- The CHAIN_MEMBER variable was created to indicate whether the outlet belongs to a chain (has a PRIMARY_GROUP_NUMBER).   A value of 1 represents a member, and 0 represents a non-member.

**Customer Address Dataset - Cleaning and Adjustments**
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}

# 2. Customer Address Dataset cleaning and basic transformations

# Data Structure
str(customer_address)

# Summary
summary(customer_address)

# Display the count of NA values for each variable
cat("NA count for each variable:\n")
sapply(customer_address, function(x) sum(is.na(x)))

# Display the count of NULL values for each variable
cat("NULL count for each variable:\n")
sapply(customer_address, function(x) sum(is.null(x)))

# Count duplicate rows based on 'zip' and 'full.address'
cat("Number of duplicate rows based on 'zip':", sum(duplicated(customer_address$zip)), "\n")
cat("Number of duplicate rows based on 'full.address':", sum(duplicated(customer_address$full.address)), "\n")

# Split the 'full.address' column into separate components
separated_columns <- strsplit(customer_address$full.address, ",")

# Modify the existing customer_address data frame with the separated columns in uppercase and appropriate data types
customer_address$ZIP <- as.integer(customer_address$zip)  
customer_address$CITY <- as.factor(toupper(sapply(separated_columns, `[`, 2)))
customer_address$STATE <- as.factor(toupper(sapply(separated_columns, `[`, 4)))
customer_address$COUNTY <- as.factor(toupper(sapply(separated_columns, `[`, 5)))
customer_address$REGION <- as.integer(toupper(sapply(separated_columns, `[`, 6)))
customer_address$LATITUDE <- as.numeric(sapply(separated_columns, `[`, 7))
customer_address$LONGITUDE <- as.numeric(sapply(separated_columns, `[`, 8))

# Remove the original full.address column
customer_address$full.address <- NULL
customer_address$zip <- NULL

# Remove separated_columns variable to clean up memory
rm(separated_columns)

# Find duplicate rows based on latitude and longitude
customer_address %>%
  filter(duplicated(across(c(LATITUDE, LONGITUDE))) | duplicated(across(c(LATITUDE, LONGITUDE)), fromLast = TRUE)) %>%
  arrange(LATITUDE)

# Example:
#40574	Lexington	KY	Fayette	67	38.0283	-84.4715
#40575	Lexington	KY	Fayette	67	38.0283	-84.4715
#40576	Lexington	KY	Fayette	67	38.0283	-84.4715

```

- The address was split into new columns for each component.  

- The dataset does not contain customers' actual addresses but will be used for data aggregation to support customer     segmentation. It includes 145 rows with identical geographic coordinates; however, no ZIP codes are duplicated.

**Transactionl Dataset - Cleaning and Adjustments**
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}

# 3. Transactional Dataset cleaning and basic transformations

# Data Structure
str(op_data)

# Transaction date format
op_data$TRANSACTION_DATE <- as.Date(op_data$TRANSACTION_DATE, format = "%m/%d/%Y")

# Convert the ORDER_TYPE column to a factor
op_data$ORDER_TYPE <- as.factor(op_data$ORDER_TYPE)

# Summary
summary(op_data)

# Display the count of NA values for each variable
cat("NA count for each variable:\n")
sapply(op_data, function(x) sum(is.na(x)))

#  Order type - convert nulls in OTHERs
op_data$ORDER_TYPE <- replace(op_data$ORDER_TYPE, op_data$ORDER_TYPE == "null", "OTHER")

# Days after the transaction by reference date
op_data$DAYS_AFTER <- as.Date(reference_date) - op_data$TRANSACTION_DATE

# Convert DAYS_AFTER to numeric
op_data$DAYS_AFTER <- as.numeric(op_data$DAYS_AFTER)

# Transactions with no values for cases and gallons
# Filter the rows where all the values are zero
qtd_check <- op_data %>%
  filter(ORDERED_CASES == 0 & LOADED_CASES == 0 & DELIVERED_CASES == 0 &
         ORDERED_GALLONS == 0 & LOADED_GALLONS == 0 & DELIVERED_GALLONS == 0)

# Display the filtered rows interactively with DT
datatable(qtd_check, options = list(pageLength = 10, autoWidth = TRUE))

# Remove rows where all case and gallon values are zero
op_data <- op_data %>%
  filter(!(ORDERED_CASES == 0 & LOADED_CASES == 0 & DELIVERED_CASES == 0 &
           ORDERED_GALLONS == 0 & LOADED_GALLONS == 0 & DELIVERED_GALLONS == 0))

# Negative Cases Deliveries
# Creating RETURNED_CASES column based on DELIVERED_CASES values  
op_data$RETURNED_CASES <- ifelse(op_data$DELIVERED_CASES < 0, -op_data$DELIVERED_CASES, 0)

# Replacing negative values in DELIVERED_CASES with 0  
op_data$DELIVERED_CASES[op_data$DELIVERED_CASES < 0] <- 0

# Negative Gallons Deliveries
# Creating RETURNED_CASES column based on DELIVERED_GALLONS values  
op_data$RETURNED_GALLONS <- ifelse(op_data$DELIVERED_GALLONS < 0, -op_data$DELIVERED_GALLONS, 0)

# Replacing negative values in DELIVERED_GALLONS with 0  
op_data$DELIVERED_GALLONS[op_data$DELIVERED_GALLONS < 0] <- 0

# Transactions with no values for Delivered or returned cases and gallons
# Filter the rows where all the values are zero
qtd_check <- op_data %>%
  filter(DELIVERED_CASES == 0 & RETURNED_CASES == 0 & 
         DELIVERED_GALLONS == 0 & RETURNED_GALLONS == 0)

# Display the filtered rows interactively with DT
datatable(qtd_check, options = list(pageLength = 10, autoWidth = TRUE))

# Classifying transactions based on delivery and return information
op_data <- op_data %>%
  mutate(
    DLV_TYPE = case_when(
      DELIVERED_CASES > 0 & DELIVERED_GALLONS == 0 ~ "CASES",  # Delivered cases, no gallons
      DELIVERED_GALLONS > 0 & DELIVERED_CASES == 0 ~ "GALLONS",  # Delivered gallons, no cases
      DELIVERED_CASES > 0 & DELIVERED_GALLONS > 0 ~ "BOTH",  # Delivered both cases and gallons
      RETURNED_CASES > 0 & RETURNED_GALLONS == 0 ~ "RETURN_CASES",  # Returned cases, no gallons
      RETURNED_GALLONS > 0 & RETURNED_CASES == 0 ~ "RETURN_GALLONS",  # Returned gallons, no cases
      RETURNED_CASES > 0 & RETURNED_GALLONS > 0 ~ "RETURN_BOTH",  # Returned both cases and gallons
      TRUE ~ "ORDER_LOAD")) %>%  # Order and/or load transactions
  mutate(DLV_TYPE = factor(DLV_TYPE))  # Convert to factor

# Remove temporary variables and data frames
rm(qtd_check)

# Summary DLV_TYPE
summary(op_data$DLV_TYPE)

```


- 11,131 null values in the ORDER_TYPE column were replaced with "OTHER."

- The DAYS_AFTER column was added to track the number of days since the transaction, up to February 2, 2025.

- 483 rows with zero values in ORDERED, LOADED, and DELIVERED CASES and GALLONS will be removed from the dataset.

- Negative values in DELIVERED_CASES and DELIVERED_GALLONS have been moved to new columns (RETURNED_CASES and            RETURNED_GALLONS), and the original columns were set to zero.

- 30,965 transactions are related to order and/or load but do not have delivery or return data. These will be            classified as "order_load" in the DLV_TYPE column.


### 1.1 Combined Dataset Driven by Transactions


During the exploration, we concluded that combining all available data would be the best approach for subsequent analyses. Two files will be created: one preserving individual transactions and another compiling information by customer. Both will be used later in this EDA.

The PROFILE DATA contains exactly 1801 unique ZIP CODES, which will be merged with the same number of unique ZIP CODES from the CUSTOMER ADDRESS dataset. It's important to note that there are ZIP CODES with the same geographic coordinates, and thus the reliability for these cases is lower.

As mentioned earlier, the number of unique CUSTOMER NUMBERS in the PROFILE DATA (now referred to as FULL DATA) is greater than in the TRANSACTIONS dataset. Only the information for customers present in the TRANSACTIONS dataset will be merged.

```{r, results='hide'}
# Merge customer_address with profile_data using ZIP_CODE
full_data <- profile_data %>%
  left_join(customer_address, by = c("ZIP_CODE" = "ZIP"))

# Check the number of unique CUSTOMER_NUMBER in full_data and op_data
print(length(unique(full_data$CUSTOMER_NUMBER)))
print(length(unique(op_data$CUSTOMER_NUMBER)))

# Filter full_data to keep only CUSTOMER_NUMBERs that are also in op_data, and merge with op_data
full_data <- full_data %>%
  filter(CUSTOMER_NUMBER %in% op_data$CUSTOMER_NUMBER) %>%
  left_join(op_data, by = "CUSTOMER_NUMBER")
```
Below are the first few rows of the combined dataset.

```{r}
# Display the first few rows of the combined dataset
#head(full_data)
```

The variable LOCAL_FOUNT_ONLY will be created to identify whether the transaction's customer belongs to the "Local Market Partners Buying Fountain Only" group—customers who purchase only fountain drinks, excluding CO2, cans, or bottles. It will be assigned a value of 1 if the customer belongs to this group and 0 otherwise.

```{r}
# Aggregate total delivered cases and gallons per customer
customer_summary <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(
    TOTAL_DELIVERED_CASES = sum(DELIVERED_CASES),
    TOTAL_DELIVERED_GALLONS = sum(DELIVERED_GALLONS),
    LOCAL_MARKET_PARTNER = max(LOCAL_MARKET_PARTNER),
    CO2_CUSTOMER = max(CO2_CUSTOMER),
    .groups = "drop")

# Classify customers based on aggregated values
customer_summary <- customer_summary %>%
  mutate(LOCAL_FOUNT_ONLY = case_when(
    LOCAL_MARKET_PARTNER == 1 & CO2_CUSTOMER == 0 & 
      TOTAL_DELIVERED_GALLONS > 0 & TOTAL_DELIVERED_CASES == 0 ~ 1L,
    TRUE ~ 0L))

# Merge back to original data
full_data <- full_data %>%
  left_join(dplyr::select(customer_summary, CUSTOMER_NUMBER, LOCAL_FOUNT_ONLY), by = "CUSTOMER_NUMBER")

# Remove temporary variables and data frames
rm(customer_summary)
```

The code below will create a table for an initial overview of the customer types.

```{r}
# Aggregate data by LOCAL_FOUNT_ONLY
summary_data <- full_data %>%
  group_by(LOCAL_FOUNT_ONLY) %>%
  summarise(
    customers = n_distinct(CUSTOMER_NUMBER),  # Count unique customers
    transactions = n(),  # Count transactions for this group
    qtd_cas = sum(DELIVERED_CASES),  # Total delivered cases
    qtd_gal = sum(DELIVERED_GALLONS),  # Total delivered gallons
    total_qtd = sum(DELIVERED_CASES) + sum(DELIVERED_GALLONS),  # Total volume (cases + gallons)
    .groups = "drop"
  ) %>%
  mutate(
    pct_cust = customers / sum(customers) * 100,  
    pct_trans = transactions / nrow(full_data) * 100,  
    pct_qtd = total_qtd / sum(total_qtd) * 100,  
    pct_gal = qtd_gal / sum(qtd_gal) * 100  
  ) %>%
  rename(LFO = LOCAL_FOUNT_ONLY) %>%  # Rename the column
  mutate(
    # Formatting numbers with comma separator, rounding before formatting with commas
    customers = format(customers, big.mark = ",", scientific = FALSE),
    transactions = format(transactions, big.mark = ",", scientific = FALSE),
    qtd_cas = format(round(qtd_cas, 0), big.mark = ",", scientific = FALSE),  
    qtd_gal = format(round(qtd_gal, 0), big.mark = ",", scientific = FALSE),  
    total_qtd = format(round(total_qtd, 0), big.mark = ",", scientific = FALSE),  
    pct_cust = round(pct_cust, 1),
    pct_trans = round(pct_trans, 1),
    pct_qtd = round(pct_qtd, 1),
    pct_gal = round(pct_gal, 1)
  )

# Add the total row (LFO = "Total")
total_row <- summary_data %>%
  summarise(
    LFO = "Total",
    customers = sum(as.numeric(gsub(",", "", customers))),
    transactions = sum(as.numeric(gsub(",", "", transactions))),
    qtd_cas = sum(as.numeric(gsub(",", "", qtd_cas))),
    qtd_gal = sum(as.numeric(gsub(",", "", qtd_gal))),
    total_qtd = sum(as.numeric(gsub(",", "", total_qtd))),
    pct_cust = 100,
    pct_trans = 100,
    pct_qtd = 100,
    pct_gal = 100
  ) %>%
  mutate(
    customers = format(customers, big.mark = ",", scientific = FALSE),
    transactions = format(transactions, big.mark = ",", scientific = FALSE),
    qtd_cas = format(round(qtd_cas, 0), big.mark = ",", scientific = FALSE),
    qtd_gal = format(round(qtd_gal, 0), big.mark = ",", scientific = FALSE),
    total_qtd = format(round(total_qtd, 0), big.mark = ",", scientific = FALSE),
    pct_cust = round(pct_cust, 1),
    pct_trans = round(pct_trans, 1),
    pct_qtd = round(pct_qtd, 1),
    pct_gal = round(pct_gal, 1)
  )

# Convert LFO to character to ensure consistency
summary_data <- summary_data %>% mutate(LFO = as.character(LFO))
total_row <- total_row %>% mutate(LFO = as.character(LFO))

# Combine the summary data with the total row
combined_data <- bind_rows(summary_data, total_row)

# Reorder the columns
combined_data <- combined_data[, c("LFO", "customers", "pct_cust", "transactions", "pct_trans", "qtd_cas", "qtd_gal", "pct_gal", "total_qtd", "pct_qtd")]

# Create the combined table
combined_data %>%
  kable("html", escape = FALSE, align = "c") %>%
  kable_styling(full_width = F, position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:10, width = "6em") %>%
  row_spec(0, bold = TRUE, color = "black", background = "lightgray") %>%  # Light gray header
  add_header_above(c("Local Market Partners Fountain Only (LFO) - Delivery Quantities Overview" = 10)) %>%
  kable_paper("striped", full_width = F)

# Remove temporary variables and data frames
rm(summary_data, total_row, combined_data)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Filter the rows where LOCAL_FOUNT_ONLY == 1 and calculate the sum of DELIVERED_CASES, RETURNED_CASES, ORDERED_CASES and the count of CO2_CUSTOMER
full_data %>%
  filter(LOCAL_FOUNT_ONLY == 1) %>%
  summarise(
    total_delivered_cases = sum(DELIVERED_CASES, na.rm = TRUE),
    total_returned_cases = sum(RETURNED_CASES, na.rm = TRUE),
    total_ordered_cases = sum(ORDERED_CASES, na.rm = TRUE),
    co2_customer_count = sum(CO2_CUSTOMER == 1, na.rm = TRUE))

# Filter rows where LOCAL_FOUNT_ONLY == 1, and both RETURNED_CASES and ORDERED_CASES are greater than 0
full_data %>%
  filter(LOCAL_FOUNT_ONLY == 1, ORDERED_CASES > 0)

```

Only 4.5% of customers are Local Market Partners who do not purchase CO2 and buy only fountain drinks (LFO = 1), accounting for 3% of transactions. They consumed 5.9% of delivered gallons but represent just 1.9% of the total volume (cases + gallons).

This small group of 1,359 customers includes 83 transactions with positive ordered cases. The last order was placed on December 19, 2024, which would allow for some case deliveries to appear in transactions. Since this didn’t happen, we will classify these customers as part of the LFO group, as they consume fountain drinks (gallons), despite ordering cases.

### 1.2 Combined Dataset Driven by Outlets

The information from the combined transaction dataset (`full_data`) will now be merged by customer and named `full_data_customer`. The goal is to create a unique list of customers who have made transactions. This file will contain a large number of columns and will be used for further analysis.

```{r}
# Creating the YEAR_MONTH column to identify the periods
full_data <- full_data %>%
  mutate(YEAR_MONTH = format(as.Date(TRANSACTION_DATE), "%Y_%m"))

# Function to count transactions by period
count_transactions <- function(df, value_column, prefix) {
  df %>%
    group_by(CUSTOMER_NUMBER, YEAR_MONTH) %>%
    summarise(value_count = sum(!!sym(value_column) > 0, na.rm = TRUE), .groups = "drop") %>%
    pivot_wider(names_from = YEAR_MONTH, values_from = value_count, names_prefix = prefix, values_fill = list(value_count = 0))
}

# Counting transactions for each metric
trans_ordered_cases <- count_transactions(full_data, "ORDERED_CASES", "TRANS_ORD_CA_")
trans_ordered_gallons <- count_transactions(full_data, "ORDERED_GALLONS", "TRANS_ORD_GAL_")
trans_delivered_cases <- count_transactions(full_data, "DELIVERED_CASES", "TRANS_DLV_CA_")
trans_delivered_gallons <- count_transactions(full_data, "DELIVERED_GALLONS", "TRANS_DLV_GAL_")
trans_returned_cases <- count_transactions(full_data, "RETURNED_CASES", "TRANS_RET_CA_")
trans_returned_gallons <- count_transactions(full_data, "RETURNED_GALLONS", "TRANS_RET_GAL_")

# Function to sum the values by period
sum_transactions <- function(df, value_column, prefix) {
  df %>%
    group_by(CUSTOMER_NUMBER, YEAR_MONTH) %>%
    summarise(value_sum = sum(!!sym(value_column), na.rm = TRUE), .groups = "drop") %>%
    pivot_wider(names_from = YEAR_MONTH, values_from = value_sum, names_prefix = prefix, values_fill = list(value_sum = 0))
}

# Summing transactions for each metric
qtd_ordered_cases <- sum_transactions(full_data, "ORDERED_CASES", "QTD_ORD_CA_")
qtd_ordered_gallons <- sum_transactions(full_data, "ORDERED_GALLONS", "QTD_ORD_GAL_")
qtd_delivered_cases <- sum_transactions(full_data, "DELIVERED_CASES", "QTD_DLV_CA_")
qtd_delivered_gallons <- sum_transactions(full_data, "DELIVERED_GALLONS", "QTD_DLV_GAL_")
qtd_returned_cases <- sum_transactions(full_data, "RETURNED_CASES", "QTD_RET_CA_")
qtd_returned_gallons <- sum_transactions(full_data, "RETURNED_GALLONS", "QTD_RET_GAL_")

# Ensure the columns in column_order are present in full_data
column_order <- c("CUSTOMER_NUMBER", "PRIMARY_GROUP_NUMBER", "FREQUENT_ORDER_TYPE", 
                  "FIRST_DELIVERY_DATE", "ON_BOARDING_DATE", "LOCAL_FOUNT_ONLY","COLD_DRINK_CHANNEL", 
                  "TRADE_CHANNEL", "SUB_TRADE_CHANNEL", "LOCAL_MARKET_PARTNER", 
                  "CO2_CUSTOMER", "ZIP_CODE", "CHAIN_MEMBER", "CITY", "STATE", 
                  "COUNTY", "REGION", "LATITUDE", "LONGITUDE")

# Check if all columns exist in full_data
missing_cols <- setdiff(column_order, colnames(full_data))
if (length(missing_cols) > 0) {
  stop("The following columns are missing in full_data: ", paste(missing_cols, collapse = ", "))
}

# Count the number of transactions per customer
trans_count <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(TRANSACTIONS_DATE_COUNT = n(), .groups = "drop")

# Joining the data with the required columns in the desired order
full_data_customer <- distinct(full_data[, column_order]) %>%
  left_join(trans_count, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_ordered_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_ordered_gallons, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_delivered_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_delivered_gallons, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_returned_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(trans_returned_gallons, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_ordered_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_ordered_gallons, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_delivered_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_delivered_gallons, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_returned_cases, by = "CUSTOMER_NUMBER") %>%
  left_join(qtd_returned_gallons, by = "CUSTOMER_NUMBER")

# Rename Order Types
full_data <- full_data %>%
  mutate(ORDER_TYPE = dplyr::recode(ORDER_TYPE, 
                             "CALL CENTER" = "CALL.CENTER",
                             "MYCOKE LEGACY" = "MYCOKE.LEGACY",
                             "SALES REP" = "SALES.REP"))

# Count transactions by ORDER_TYPE
order_type_count <- full_data %>%
  group_by(CUSTOMER_NUMBER, ORDER_TYPE) %>%
  summarise(order_type_count = n(), .groups = "drop") %>%
  pivot_wider(names_from = ORDER_TYPE, values_from = order_type_count, names_prefix = "OT_", values_fill = list(order_type_count = 0))

# Count transactions by DLV_TYPE
dlv_type_count <- full_data %>%
  group_by(CUSTOMER_NUMBER, DLV_TYPE) %>%
  summarise(dlv_type_count = n(), .groups = "drop") %>%
  pivot_wider(names_from = DLV_TYPE, values_from = dlv_type_count, names_prefix = "DLVT_", values_fill = list(dlv_type_count = 0))

# Join with the full_data_customer to ensure ORDER_TYPE and DLV_TYPE columns are added
full_data_customer <- full_data_customer %>%
  left_join(order_type_count, by = "CUSTOMER_NUMBER") %>%
  left_join(dlv_type_count, by = "CUSTOMER_NUMBER")

# Adding the requested summary columns
full_data_customer <- full_data_customer %>%
  mutate(TOTAL_CASES_ORDERED = rowSums(full_data_customer[, grep("^QTD_ORD_CA_", names(full_data_customer))]),
         TOTAL_CASES_DELIVERED = rowSums(full_data_customer[, grep("^QTD_DLV_CA_", names(full_data_customer))]),
         TOTAL_GALLONS_ORDERED = rowSums(full_data_customer[, grep("^QTD_ORD_GAL_", names(full_data_customer))]),
         TOTAL_GALLONS_DELIVERED = rowSums(full_data_customer[, grep("^QTD_DLV_GAL_", names(full_data_customer))]),
         TOTAL_CASES_RETURNED = rowSums(full_data_customer[, grep("^QTD_RET_CA_", names(full_data_customer))]),
         TOTAL_GALLONS_RETURNED = rowSums(full_data_customer[, grep("^QTD_RET_GAL_", names(full_data_customer))]))

# Ensuring column order
ot_columns <- colnames(order_type_count)[-1]
dlvt_columns <- colnames(dlv_type_count)[-1]
summary_columns <- c("TOTAL_CASES_ORDERED", "TOTAL_CASES_DELIVERED", "TOTAL_GALLONS_ORDERED", "TOTAL_GALLONS_DELIVERED", "TOTAL_CASES_RETURNED", "TOTAL_GALLONS_RETURNED")
transaction_columns <- grep("^TRANS_", colnames(full_data_customer), value = TRUE)
quantity_columns <- grep("^QTD_", colnames(full_data_customer), value = TRUE)
ordered_columns <- c(column_order, "TRANSACTIONS_DATE_COUNT", ot_columns, dlvt_columns, summary_columns, sort(transaction_columns), sort(quantity_columns))

# Reordering full_data_customer
full_data_customer <- full_data_customer[, ordered_columns]

# Replacing NAs with 0 in transaction and quantity columns
full_data_customer[is.na(full_data_customer)] <- 0

# Extra variables

# Define reference date
ref_date <- as.Date("2025-02-01")

# 1. DAYS_FIRST_DLV
full_data_customer$DAYS_FIRST_DLV <- as.numeric(difftime(ref_date, full_data_customer$FIRST_DELIVERY_DATE, units = "days"))

# 2. DAYS_ONBOARDING
full_data_customer$DAYS_ONBOARDING <- as.numeric(difftime(ref_date, full_data_customer$ON_BOARDING_DATE, units = "days"))

# 3. Average transactions per month
# Replace NA with 0 for missing transactions
full_data_customer[is.na(full_data_customer)] <- 0

# Calculate the average transaction per month
cols_to_average_dlv <- grep("^TRANS_DLV_CA", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_dlv] <- lapply(full_data_customer[cols_to_average_dlv], as.numeric)
full_data_customer$AVG_TRANS_DLV_CA_M <- rowMeans(full_data_customer[, cols_to_average_dlv], na.rm = TRUE)

cols_to_average_gal <- grep("^TRANS_DLV_GAL", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_gal] <- lapply(full_data_customer[cols_to_average_gal], as.numeric)
full_data_customer$AVG_TRANS_DLV_GAL_M <- rowMeans(full_data_customer[, cols_to_average_gal], na.rm = TRUE)

cols_to_average_ord_ca <- grep("^TRANS_ORD_CA", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_ord_ca] <- lapply(full_data_customer[cols_to_average_ord_ca], as.numeric)
full_data_customer$AVG_TRANS_ORD_CA_M <- rowMeans(full_data_customer[, cols_to_average_ord_ca], na.rm = TRUE)

cols_to_average_ord_gal <- grep("^TRANS_ORD_GAL", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_ord_gal] <- lapply(full_data_customer[cols_to_average_ord_gal], as.numeric)
full_data_customer$AVG_TRANS_ORD_GAL_M <- rowMeans(full_data_customer[, cols_to_average_ord_gal], na.rm = TRUE)

cols_to_average_ret_ca <- grep("^TRANS_RET_CA", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_ret_ca] <- lapply(full_data_customer[cols_to_average_ret_ca], as.numeric)
full_data_customer$AVG_TRANS_RET_CA_M <- rowMeans(full_data_customer[, cols_to_average_ret_ca], na.rm = TRUE)

cols_to_average_ret_gal <- grep("^TRANS_RET_GAL", names(full_data_customer), value = TRUE)
full_data_customer[cols_to_average_ret_gal] <- lapply(full_data_customer[cols_to_average_ret_gal], as.numeric)
full_data_customer$AVG_TRANS_RET_GAL_M <- rowMeans(full_data_customer[, cols_to_average_ret_gal], na.rm = TRUE)

# 4. Number of transactions per year (sum annual columns)
full_data_customer$NUM_TRANS_ORD_CA_23 <- rowSums(full_data_customer[, grep("^TRANS_ORD_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_ORD_CA_24 <- rowSums(full_data_customer[, grep("^TRANS_ORD_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_ORD_GAL_23 <- rowSums(full_data_customer[, grep("^TRANS_ORD_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_ORD_GAL_24 <- rowSums(full_data_customer[, grep("^TRANS_ORD_GAL_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_DLV_CA_23 <- rowSums(full_data_customer[, grep("^TRANS_DLV_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_DLV_CA_24 <- rowSums(full_data_customer[, grep("^TRANS_DLV_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_DLV_GAL_23 <- rowSums(full_data_customer[, grep("^TRANS_DLV_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_DLV_GAL_24 <- rowSums(full_data_customer[, grep("^TRANS_DLV_GAL_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_RET_CA_23 <- rowSums(full_data_customer[, grep("^TRANS_RET_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_RET_CA_24 <- rowSums(full_data_customer[, grep("^TRANS_RET_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_RET_GAL_23 <- rowSums(full_data_customer[, grep("^TRANS_RET_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$NUM_TRANS_RET_GAL_24 <- rowSums(full_data_customer[, grep("^TRANS_RET_GAL_2024", names(full_data_customer))], na.rm = TRUE)

# 5. Sum of quantities per year
full_data_customer$QTD_ORD_CA_2023 <- rowSums(full_data_customer[, grep("^QTD_ORD_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_ORD_GAL_2023 <- rowSums(full_data_customer[, grep("^QTD_ORD_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_ORD_CA_2024 <- rowSums(full_data_customer[, grep("^QTD_ORD_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_ORD_GAL_2024 <- rowSums(full_data_customer[, grep("^QTD_ORD_GAL_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_DLV_CA_2023 <- rowSums(full_data_customer[, grep("^QTD_DLV_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_DLV_GAL_2023 <- rowSums(full_data_customer[, grep("^QTD_DLV_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_DLV_CA_2024 <- rowSums(full_data_customer[, grep("^QTD_DLV_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_DLV_GAL_2024 <- rowSums(full_data_customer[, grep("^QTD_DLV_GAL_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_RET_CA_2023 <- rowSums(full_data_customer[, grep("^QTD_RET_CA_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_RET_GAL_2023 <- rowSums(full_data_customer[, grep("^QTD_RET_GAL_2023", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_RET_CA_2024 <- rowSums(full_data_customer[, grep("^QTD_RET_CA_2024", names(full_data_customer))], na.rm = TRUE)
full_data_customer$QTD_RET_GAL_2024 <- rowSums(full_data_customer[, grep("^QTD_RET_GAL_2024", names(full_data_customer))], na.rm = TRUE)

# 6. Create new columns for CUST_23 and CUST_24
full_data_customer$ACTIVE_23 <- ifelse((full_data_customer$QTD_DLV_CA_2023 + full_data_customer$QTD_DLV_GAL_2023) > 0, 1, 0)
full_data_customer$ACTIVE_24 <- ifelse((full_data_customer$QTD_DLV_CA_2024 + full_data_customer$QTD_DLV_GAL_2024) > 0, 1, 0)

# Display the first few rows of the combined dataset
#head(full_data_customer)

```

To avoid having to perform the entire processing again, the new datasets will be saved and reloaded whenever necessary.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Remove temporary data frames
rm(list = c("trans_ordered_cases", "trans_ordered_gallons", "trans_delivered_cases", 
            "trans_delivered_gallons", "trans_returned_cases", "trans_returned_gallons", 
            "qtd_ordered_cases", "qtd_ordered_gallons", "qtd_delivered_cases", 
            "qtd_delivered_gallons", "qtd_returned_cases", "qtd_returned_gallons", 
            "trans_count", "order_type_count", "dlv_type_count"))

# Remove temporary functions
rm(list = c("count_transactions", "sum_transactions"))

# Remove column order vector
rm(column_order)

# Remove unnecessary intermediate data frames
rm(list = c("cols_to_average_dlv", "cols_to_average_gal", "cols_to_average_ord_ca", 
            "cols_to_average_ord_gal", "cols_to_average_ret_ca", "cols_to_average_ret_gal", 
            "ref_date"))
```

### 1.3 Estimated Delivery Costs

The delivery costs will reflect estimated volumes, as they were provided based on the median price within volume ranges and by type of COLD_DRINK_CHANNEL. However, the "CONVENTIONAL" category is not listed in the file. We understand that it represents pharmacy retailers or independent local stores; therefore, for calculation purposes, we will use the same cost applied to GOODS.

```{r}
# Load the delivery cost data from the Excel file
cost_data <- read_excel("delivery_cost_data.xlsx")

# Convert 'Cold Drink Channel' to a factor
cost_data <- cost_data %>%
  mutate(COLD_DRINK_CHANNEL = factor(`Cold Drink Channel`))

# Copy rows where 'Cold Drink Channel' is "GOODS" and change its value to "CONVENTIONAL"
cost_data <- bind_rows(
  cost_data,
  cost_data %>%
    filter(COLD_DRINK_CHANNEL == "GOODS") %>%
    mutate(COLD_DRINK_CHANNEL = "CONVENTIONAL")
)

# Manually recode 'Applicable To' values
cost_data <- cost_data %>%
  mutate(`Applicable To` = ifelse(`Applicable To` == "Bottles and Cans", "CASES", 
                                  ifelse(`Applicable To` == "Fountain", "GALLONS", `Applicable To`)))

# Create RANGE_LEVEL based on 'Vol Range' and make it a factor
cost_data$RANGE_LEVEL <- factor(case_when(
  cost_data$`Vol Range` == "0 - 149" & cost_data$`Applicable To` == "CASES" ~ "RANGE_1_CASES",
  cost_data$`Vol Range` == "150 - 299" & cost_data$`Applicable To` == "CASES" ~ "RANGE_2_CASES", 
  cost_data$`Vol Range` == "300 - 449" & cost_data$`Applicable To` == "CASES" ~ "RANGE_3_CASES",
  cost_data$`Vol Range` == "450 - 599" & cost_data$`Applicable To` == "CASES" ~ "RANGE_4_CASES",
  cost_data$`Vol Range` == "600 - 749" & cost_data$`Applicable To` == "CASES" ~ "RANGE_5_CASES",
  cost_data$`Vol Range` == "750 - 899" & cost_data$`Applicable To` == "CASES" ~ "RANGE_6_CASES",
  cost_data$`Vol Range` == "900 - 1049" & cost_data$`Applicable To` == "CASES" ~ "RANGE_7_CASES",
  cost_data$`Vol Range` == "1050 - 1199" & cost_data$`Applicable To` == "CASES" ~ "RANGE_8_CASES",
  cost_data$`Vol Range` == "1200 - 1349" & cost_data$`Applicable To` == "CASES" ~ "RANGE_9_CASES",
  cost_data$`Vol Range` == "1350+" & cost_data$`Applicable To` == "CASES" ~ "RANGE_10_CASES",
  
  cost_data$`Vol Range` == "0 - 149" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_1_GALLONS",
  cost_data$`Vol Range` == "150 - 299" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_2_GALLONS", 
  cost_data$`Vol Range` == "300 - 449" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_3_GALLONS",
  cost_data$`Vol Range` == "450 - 599" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_4_GALLONS",
  cost_data$`Vol Range` == "600 - 749" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_5_GALLONS",
  cost_data$`Vol Range` == "750 - 899" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_6_GALLONS",
  cost_data$`Vol Range` == "900 - 1049" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_7_GALLONS",
  cost_data$`Vol Range` == "1050 - 1199" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_8_GALLONS",
  cost_data$`Vol Range` == "1200 - 1349" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_9_GALLONS",
  cost_data$`Vol Range` == "1350+" & cost_data$`Applicable To` == "GALLONS" ~ "RANGE_10_GALLONS"
))

# Reorder columns to keep only the desired ones: COLD_DRINK_CHANNEL, VOL_RANGE, RANGE_LEVEL, MEDIAN DELIVERY COST
cost_data <- cost_data[, c("COLD_DRINK_CHANNEL", "Vol Range", "RANGE_LEVEL", "Median Delivery Cost")]

# Check the result
#head(cost_data)
```

The necessary variables will be created to calculate the delivery costs for cases and gallons for the years 2023 and 2024 by customer.

```{r}
# Create cost range columns for each year based on quantities
full_data_customer <- full_data_customer %>%
  mutate(
    # For 2023, categorize based on quantity ranges for cases
    COST_RANGE_CA_23 = case_when(
      QTD_DLV_CA_2023 >= 0 & QTD_DLV_CA_2023 < 150 ~ "RANGE_1_CASES", 
      QTD_DLV_CA_2023 >= 150 & QTD_DLV_CA_2023 < 300 ~ "RANGE_2_CASES",
      QTD_DLV_CA_2023 >= 300 & QTD_DLV_CA_2023 < 450 ~ "RANGE_3_CASES",
      QTD_DLV_CA_2023 >= 450 & QTD_DLV_CA_2023 < 600 ~ "RANGE_4_CASES",
      QTD_DLV_CA_2023 >= 600 & QTD_DLV_CA_2023 < 750 ~ "RANGE_5_CASES",
      QTD_DLV_CA_2023 >= 750 & QTD_DLV_CA_2023 < 900 ~ "RANGE_6_CASES",
      QTD_DLV_CA_2023 >= 900 & QTD_DLV_CA_2023 < 1050 ~ "RANGE_7_CASES",
      QTD_DLV_CA_2023 >= 1050 & QTD_DLV_CA_2023 < 1200 ~ "RANGE_8_CASES",
      QTD_DLV_CA_2023 >= 1200 & QTD_DLV_CA_2023 < 1350 ~ "RANGE_9_CASES",
      QTD_DLV_CA_2023 >= 1350 ~ "RANGE_10_CASES", 
      TRUE ~ NA_character_),
    
    # For 2024, categorize based on quantity ranges for cases
    COST_RANGE_CA_24 = case_when(
      QTD_DLV_CA_2024 >= 0 & QTD_DLV_CA_2024 < 150 ~ "RANGE_1_CASES",
      QTD_DLV_CA_2024 >= 150 & QTD_DLV_CA_2024 < 300 ~ "RANGE_2_CASES",
      QTD_DLV_CA_2024 >= 300 & QTD_DLV_CA_2024 < 450 ~ "RANGE_3_CASES",
      QTD_DLV_CA_2024 >= 450 & QTD_DLV_CA_2024 < 600 ~ "RANGE_4_CASES",
      QTD_DLV_CA_2024 >= 600 & QTD_DLV_CA_2024 < 750 ~ "RANGE_5_CASES",
      QTD_DLV_CA_2024 >= 750 & QTD_DLV_CA_2024 < 900 ~ "RANGE_6_CASES",
      QTD_DLV_CA_2024 >= 900 & QTD_DLV_CA_2024 < 1050 ~ "RANGE_7_CASES",
      QTD_DLV_CA_2024 >= 1050 & QTD_DLV_CA_2024 < 1200 ~ "RANGE_8_CASES",
      QTD_DLV_CA_2024 >= 1200 & QTD_DLV_CA_2024 < 1350 ~ "RANGE_9_CASES",
      QTD_DLV_CA_2024 >= 1350 ~ "RANGE_10_CASES",
      TRUE ~ NA_character_),
    
    # For 2023, categorize based on quantity ranges for gallons
    COST_RANGE_GAL_23 = case_when(
      QTD_DLV_GAL_2023 >= 0 & QTD_DLV_GAL_2023 < 150 ~ "RANGE_1_GALLONS", 
      QTD_DLV_GAL_2023 >= 150 & QTD_DLV_GAL_2023 < 300 ~ "RANGE_2_GALLONS",
      QTD_DLV_GAL_2023 >= 300 & QTD_DLV_GAL_2023 < 450 ~ "RANGE_3_GALLONS",
      QTD_DLV_GAL_2023 >= 450 & QTD_DLV_GAL_2023 < 600 ~ "RANGE_4_GALLONS",
      QTD_DLV_GAL_2023 >= 600 & QTD_DLV_GAL_2023 < 750 ~ "RANGE_5_GALLONS",
      QTD_DLV_GAL_2023 >= 750 & QTD_DLV_GAL_2023 < 900 ~ "RANGE_6_GALLONS",
      QTD_DLV_GAL_2023 >= 900 & QTD_DLV_GAL_2023 < 1050 ~ "RANGE_7_GALLONS",
      QTD_DLV_GAL_2023 >= 1050 & QTD_DLV_GAL_2023 < 1200 ~ "RANGE_8_GALLONS",
      QTD_DLV_GAL_2023 >= 1200 & QTD_DLV_GAL_2023 < 1350 ~ "RANGE_9_GALLONS",
      QTD_DLV_GAL_2023 >= 1350 ~ "RANGE_10_GALLONS", 
      TRUE ~ NA_character_),
    
    # For 2024, categorize based on quantity ranges for gallons
    COST_RANGE_GAL_24 = case_when(
      QTD_DLV_GAL_2024 >= 0 & QTD_DLV_GAL_2024 < 150 ~ "RANGE_1_GALLONS",
      QTD_DLV_GAL_2024 >= 150 & QTD_DLV_GAL_2024 < 300 ~ "RANGE_2_GALLONS",
      QTD_DLV_GAL_2024 >= 300 & QTD_DLV_GAL_2024 < 450 ~ "RANGE_3_GALLONS",
      QTD_DLV_GAL_2024 >= 450 & QTD_DLV_GAL_2024 < 600 ~ "RANGE_4_GALLONS",
      QTD_DLV_GAL_2024 >= 600 & QTD_DLV_GAL_2024 < 750 ~ "RANGE_5_GALLONS",
      QTD_DLV_GAL_2024 >= 750 & QTD_DLV_GAL_2024 < 900 ~ "RANGE_6_GALLONS",
      QTD_DLV_GAL_2024 >= 900 & QTD_DLV_GAL_2024 < 1050 ~ "RANGE_7_GALLONS",
      QTD_DLV_GAL_2024 >= 1050 & QTD_DLV_GAL_2024 < 1200 ~ "RANGE_8_GALLONS",
      QTD_DLV_GAL_2024 >= 1200 & QTD_DLV_GAL_2024 < 1350 ~ "RANGE_9_GALLONS",
      QTD_DLV_GAL_2024 >= 1350 ~ "RANGE_10_GALLONS",
      TRUE ~ NA_character_ ))


# First join for UNIT_COST_CA_23
full_data_customer <- full_data_customer %>%
  left_join(cost_data %>% dplyr::select(COLD_DRINK_CHANNEL, RANGE_LEVEL, `Median Delivery Cost`), 
            by = c("COLD_DRINK_CHANNEL" = "COLD_DRINK_CHANNEL", 
                   "COST_RANGE_CA_23" = "RANGE_LEVEL")) %>%
  mutate(UNIT_COST_CA_23 = `Median Delivery Cost`) %>%
  dplyr::select(-`Median Delivery Cost`)  # Remove unwanted column

# Second join for UNIT_COST_CA_24
full_data_customer <- full_data_customer %>%
  left_join(cost_data %>% dplyr::select(COLD_DRINK_CHANNEL, RANGE_LEVEL, `Median Delivery Cost`), 
            by = c("COLD_DRINK_CHANNEL" = "COLD_DRINK_CHANNEL", 
                   "COST_RANGE_CA_24" = "RANGE_LEVEL")) %>%
  mutate(UNIT_COST_CA_24 = `Median Delivery Cost`) %>%
  dplyr::select(-`Median Delivery Cost`)  # Remove unwanted column

# Third join for UNIT_COST_GAL_23
full_data_customer <- full_data_customer %>%
  left_join(cost_data %>% dplyr::select(COLD_DRINK_CHANNEL, RANGE_LEVEL, `Median Delivery Cost`), 
            by = c("COLD_DRINK_CHANNEL" = "COLD_DRINK_CHANNEL", 
                   "COST_RANGE_GAL_23" = "RANGE_LEVEL")) %>%
  mutate(UNIT_COST_GAL_23 = `Median Delivery Cost`) %>%
  dplyr::select(-`Median Delivery Cost`)  # Remove unwanted column

# Fourth join for UNIT_COST_GAL_24
full_data_customer <- full_data_customer %>%
  left_join(cost_data %>% dplyr::select(COLD_DRINK_CHANNEL, RANGE_LEVEL, `Median Delivery Cost`), 
            by = c("COLD_DRINK_CHANNEL" = "COLD_DRINK_CHANNEL", 
                   "COST_RANGE_GAL_24" = "RANGE_LEVEL")) %>%
  mutate(UNIT_COST_GAL_24 = `Median Delivery Cost`) %>%
  dplyr::select(-`Median Delivery Cost`)  # Remove unwanted column

# Calculating delivery costs for each year and drink type
full_data_customer <- full_data_customer %>%
  mutate(
    COST_CA_23 = QTD_DLV_CA_2023 * UNIT_COST_CA_23,
    COST_CA_24 = QTD_DLV_CA_2024 * UNIT_COST_CA_24,
    COST_GAL_23 = QTD_DLV_GAL_2023 * UNIT_COST_GAL_23,
    COST_GAL_24 = QTD_DLV_GAL_2024 * UNIT_COST_GAL_24 )

# Format unit costs and costs to two decimal places
full_data_customer <- full_data_customer %>%
  mutate(
    UNIT_COST_CA_23 = round(UNIT_COST_CA_23, 8),
    UNIT_COST_CA_24 = round(UNIT_COST_CA_24, 8),
    UNIT_COST_GAL_23 = round(UNIT_COST_GAL_23, 8),
    UNIT_COST_GAL_24 = round(UNIT_COST_GAL_24, 8),
    COST_CA_23 = round(COST_CA_23, 8),
    COST_CA_24 = round(COST_CA_24, 8),
    COST_GAL_23 = round(COST_GAL_23, 8),
    COST_GAL_24 = round(COST_GAL_24, 8))
```

The table below presents the information that constitutes the calculation of the delivery cost per customer.

```{r, warning=FALSE}
# Costs table
summary_table <- as.data.table(full_data_customer)[, .(
  CUSTOMER_NUMBER, COLD_DRINK_CHANNEL,
  QTD_DLV_CA_2023 = round(QTD_DLV_CA_2023, 0), 
  QTD_DLV_CA_2024 = round(QTD_DLV_CA_2024, 0), 
  QTD_DLV_GAL_2023 = round(QTD_DLV_GAL_2023, 0), 
  QTD_DLV_GAL_2024 = round(QTD_DLV_GAL_2024, 0),
  COST_RANGE_CA_23, COST_RANGE_CA_24, COST_RANGE_GAL_23, COST_RANGE_GAL_24,
  UNIT_COST_CA_23 = round(UNIT_COST_CA_23, 2), 
  UNIT_COST_CA_24 = round(UNIT_COST_CA_24, 2), 
  UNIT_COST_GAL_23 = round(UNIT_COST_GAL_23, 2), 
  UNIT_COST_GAL_24 = round(UNIT_COST_GAL_24, 2),
  COST_CA_23 = round(COST_CA_23, 2), 
  COST_CA_24 = round(COST_CA_24, 2), 
  COST_GAL_23 = round(COST_GAL_23, 2), 
  COST_GAL_24 = round(COST_GAL_24, 2))]

# Display the table interactively
datatable(summary_table, options = list(
pageLength = 10, scrollX = TRUE, scrollY = TRUE))
```

All costs are being calculated correctly. At this moment, percentage variations for the number of operations, demands, and costs have not been generated because not all customers have a history for 2023 and 2024, which prevents such calculations. However, we will later identify ways to quantify the growth of each customer.




### 1.4 Target Variables: Initial Assumptions



```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Adding new columns to full_data and full_data_customer

# QTD delivered
full_data_customer$QTD_DLV_CA_GAL_2023 <- full_data_customer$QTD_DLV_CA_2023 + full_data_customer$QTD_DLV_GAL_2023
full_data_customer$QTD_DLV_CA_GAL_2024 <- full_data_customer$QTD_DLV_CA_2024 + full_data_customer$QTD_DLV_GAL_2024

# Creating the QTD_DLV_TOTAL variable
full_data_customer$QTD_DLV_TOTAL <- full_data_customer$QTD_DLV_CA_GAL_2023 + full_data_customer$QTD_DLV_CA_GAL_2024

# Create the TOTAL_COST_CA_GAL column
full_data_customer$TOTAL_COST_CA_GAL <- full_data_customer$COST_CA_23 + 
                                        full_data_customer$COST_CA_24 + 
                                        full_data_customer$COST_GAL_23 + 
                                        full_data_customer$COST_GAL_24




# Adding cost-related columns from full_data_customer to full_data based on CUSTOMER_NUMBER
full_data$UNIT_COST_CA_23 <- as.numeric(full_data_customer$UNIT_COST_CA_23[match(full_data$CUSTOMER_NUMBER, full_data_customer$CUSTOMER_NUMBER)])

full_data$UNIT_COST_GAL_23 <- as.numeric(full_data_customer$UNIT_COST_GAL_23[match(full_data$CUSTOMER_NUMBER, full_data_customer$CUSTOMER_NUMBER)])

full_data$UNIT_COST_CA_24 <- as.numeric(full_data_customer$UNIT_COST_CA_24[match(full_data$CUSTOMER_NUMBER, full_data_customer$CUSTOMER_NUMBER)])

full_data$UNIT_COST_GAL_24 <- as.numeric(full_data_customer$UNIT_COST_GAL_24[match(full_data$CUSTOMER_NUMBER, full_data_customer$CUSTOMER_NUMBER)])



# Creating the DLV_COST_CA column
full_data$DLV_COST_CA <- ifelse(full_data$YEAR == 2023, 
                                full_data$DELIVERED_CASES * full_data$UNIT_COST_CA_23, 
                                ifelse(full_data$YEAR == 2024, 
                                       full_data$DELIVERED_CASES * full_data$UNIT_COST_CA_24, NA))

# Creating the DLV_COST_GAL column
full_data$DLV_COST_GAL <- ifelse(full_data$YEAR == 2023, 
                                 full_data$DELIVERED_GALLONS * full_data$UNIT_COST_GAL_23, 
                                 ifelse(full_data$YEAR == 2024, 
                                        full_data$DELIVERED_GALLONS * full_data$UNIT_COST_GAL_24, NA))

# Creating the DLV_COST_TOTAL column
full_data$DLV_COST_TOTAL <- full_data$DLV_COST_CA + full_data$DLV_COST_GAL

```


### 1.5 - Demand Threshold

We will calculate the average annual consumption per customer and classify them based on whether they exceed the threshold of 400 units (cases plus gallons).

```{r}

# Calculating the average
full_data_customer$AVG_ANNUAL_CONSUMP <- round((full_data_customer$QTD_DLV_CA_GAL_2023 + full_data_customer$QTD_DLV_CA_GAL_2024) / 2, 1)

# Creating the THRESHOLD_REACH variable
full_data_customer$THRESHOLD_REACH <- ifelse(full_data_customer$AVG_ANNUAL_CONSUMP < 400, 0, 1)

```


### 1.6 - Fleet Assignment

Customers who exceed 400 units annually will be assigned to Red Trucks, while the remaining customers will be allocated to White Trucks.

```{r}
# Create the FLEET_TYPE column based on THRESHOLD_REACH only
full_data_customer$FLEET_TYPE <- ifelse(full_data_customer$THRESHOLD_REACH == 1, 
                                         "RED TRUCK", 
                                         "WHITE TRUCK")

# Group and calculate the number of customers by FLEET_TYPE and LOCAL_FOUNT_ONLY
summary_fleet_type <- full_data_customer %>%
  group_by(LOCAL_FOUNT_ONLY, FLEET_TYPE) %>%
  summarise(
    total_customers = n(),
    .groups = "drop"
  )

# Calculate percentage of customers within each LOCAL_FOUNT_ONLY group separately
summary_fleet_type <- summary_fleet_type %>%
  group_by(LOCAL_FOUNT_ONLY) %>%
  mutate(
    pct_customers = total_customers / sum(total_customers) * 100  # Calculate the percentage within each LOCAL_FOUNT_ONLY group
  )

# Transform data into long format for percentages
summary_fleet_type_long <- summary_fleet_type %>%
  pivot_longer(
    cols = starts_with("pct_"),
    names_to = "metric",
    values_to = "percentage"
  ) %>%
  mutate(
    metric = factor(metric, 
                    levels = c("pct_customers"),
                    labels = c("Percentage of Customers"))
  )

# Ensure LOCAL_FOUNT_ONLY and FLEET_TYPE are factors
summary_fleet_type_long$LOCAL_FOUNT_ONLY <- factor(summary_fleet_type_long$LOCAL_FOUNT_ONLY, levels = c("0", "1"))
summary_fleet_type_long$FLEET_TYPE <- factor(summary_fleet_type_long$FLEET_TYPE, levels = c("RED TRUCK", "WHITE TRUCK"))
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Convert to FLEET_TYPE to factor
full_data_customer$FLEET_TYPE <- as.factor(full_data_customer$FLEET_TYPE)

# Add FLEET_TYPE to the full_customer (by transactions dataset)
full_data$FLEET_TYPE <- full_data_customer$FLEET_TYPE[match(full_data$CUSTOMER_NUMBER, full_data_customer$CUSTOMER_NUMBER)]

# List all variables in the environment
all_vars <- ls()

# Exclude 'full_data', 'full_data_customer', and the new variables from removal
vars_to_keep <- c("full_data", "full_data_customer", "cost_data", "customer_address", 
                  "mydir", "one_seed", "op_data", "profile_data", "reference_date","custom_palette")

# Get the variables to remove
vars_to_remove <- setdiff(all_vars, vars_to_keep)

# Remove the temporary data frames
rm(list = vars_to_remove)

# Clean up by removing 'all_vars' and 'vars_to_remove'
rm(all_vars, vars_to_remove)

```




```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Save full_data as CSV
#write.csv(full_data, file = "full_data.csv", row.names = FALSE)

# Save full_data_customer as CSV
#write.csv(full_data_customer, file = "full_data_customer.csv", row.names = FALSE)

# Load the full_data CSV
#full_data <- read.csv(file = "full_data.csv", sep = ",", stringsAsFactors = FALSE)

# Load the full_data_customer CSV
#full_data_customer <- read.csv(file = "full_data_customer.csv", sep = ",", stringsAsFactors = FALSE)

```

## 7. Feature Engineering

### 7.1 Census Data

To provide more detailed information about the locations of each store, we will be adding data obtained from the 2023 Census. 

Although we have 145 instances of addresses (out of 1,801) with the same coordinates for different ZIP codes, we have decided to use the coordinates. This choice was made because our attempt to retrieve Census data based on ZIP codes was less effective. Different customers, even those sharing the same ZIP code, can have the same coordinates when located in shopping centers or other areas with multiple stores.

Below are the descriptions of the data we will import:

```{r, out.height=2}
# Creating the data for the table
census_data <- tibble(
  variable = c(
    "MED_HH_INC", "GINI_IDX", "PER_CAP_INC", "MED_HOME_VAL", "POV_POP", 
    "INC_LVL_1", "INC_LVL_2", "INC_LVL_3", "INC_LVL_4", "INC_LVL_5", 
    "INC_LVL_6", "INC_LVL_7", "INC_LVL_8", "INC_LVL_9", "INC_LVL_10", 
    "INC_LVL_11", "INC_LVL_12", "INC_LVL_13", "INC_LVL_14", "INC_LVL_15", 
    "INC_LVL_16", "TOT_HOUS_UNITS", "VAC_HOUS_UNITS", "MED_GROSS_RENT", "BACH_DEG", 
    "MAST_DEG", "DOC_DEG", "UNEMP_POP", "EMP_POP", "TOT_WORK_POP", 
    "SNAP_HH", "MED_FAM_INC", "TOT_POP", "MALE_POP", "FEMALE_POP", 
    "COMMUTE_POP", "COMMUTE_POP_DRIVE"
  ),
  description = c(
    "Median household income", "Gini index of income inequality", 
    "Per capita income", "Median home value", "Population below poverty", 
    "Income less than $10,000", "$10,000 to $14,999", "$15,000 to $19,999", 
    "$20,000 to $24,999", "$25,000 to $29,999", "$30,000 to $34,999", 
    "$35,000 to $39,999", "$40,000 to $44,999", "$45,000 to $49,999", 
    "$50,000 to $59,999", "$60,000 to $74,999", "$75,000 to $99,999", 
    "$100,000 to $124,999", "$125,000 to $149,999", "$150,000 to $199,999", 
    "$200,000 or more", "Total housing units", "Vacant housing units", 
    "Median gross rent", "Bachelor's degree holders", "Master's degree holders", 
    "Doctoral degree holders", "Unemployed population", "Employed population", 
    "Total working population", "Food stamp households", "Median family income", 
    "Total population", "Male population", "Female population", 
    "Total commuter population", "Total commuter population driving"
  )
)

# Table
kable(census_data, 
      col.names = c("Variable", "Description"), 
      caption = "List of Census Variables and Descriptions", 
      align = c("l", "l"))
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}

# Census Bureau API key
#census_api_key(" ", install = TRUE)

# Create a copy of full_data_customer with only the relevant columns
data_sf <- full_data_customer %>%
  dplyr::select(CUSTOMER_NUMBER, LONGITUDE, LATITUDE)

# Convert customer data to sf object
data_sf <- data_sf %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326)

# Ensure the 'census_variables' object is defined
census_variables <- tibble(
  code = c(
    "B19013_001", "B19083_001", "B19301_001", "B25077_001", "B17001_002", 
    "B19001_002", "B19001_003", "B19001_004", "B19001_005", "B19001_006", 
    "B19001_007", "B19001_008", "B19001_009", "B19001_010", "B19001_011", 
    "B19001_012", "B19001_013", "B19001_014", "B19001_015", "B19001_016", 
    "B19001_017", "B25001_001", "B25002_003", "B25064_001", "B15003_017", 
    "B15003_022", "B15003_025", "B23025_005", "B23025_004", "B24011_001", 
    "B22001_002", "B19058_001", "B01003_001", "B01001_002", "B01001_026", 
    "B08006_001", "B08006_002"
  ),
  description = c(
    "MED_HH_INC", "GINI_IDX", "PER_CAP_INC", "MED_HOME_VAL", "POV_POP", 
    "INC_LVL_1", "INC_LVL_2", "INC_LVL_3", "INC_LVL_4", "INC_LVL_5", 
    "INC_LVL_6", "INC_LVL_7", "INC_LVL_8", "INC_LVL_9", "INC_LVL_10", 
    "INC_LVL_11", "INC_LVL_12", "INC_LVL_13", "INC_LVL_14", "INC_LVL_15", 
    "INC_LVL_16", "TOT_HOUS_UNITS", "VAC_HOUS_UNITS", "MED_GROSS_RENT", "BACH_DEG", 
    "MAST_DEG", "DOC_DEG", "UNEMP_POP", "EMP_POP", "TOT_WORK_POP", 
    "SNAP_HH", "MED_FAM_INC", "TOT_POP", "MALE_POP", "FEMALE_POP", 
    "COMMUTE_POP", "COMMUTE_POP_DRIVE"
  ),
  full_description = c(
    "Median household income", "Gini index of income inequality", 
    "Per capita income", "Median home value", "Population below poverty", 
    "Income less than $10,000", "$10,000 to $14,999", "$15,000 to $19,999", 
    "$20,000 to $24,999", "$25,000 to $29,999", "$30,000 to $34,999", 
    "$35,000 to $39,999", "$40,000 to $44,999", "$45,000 to $49,999", 
    "$50,000 to $59,999", "$60,000 to $74,999", "$75,000 to $99,999", 
    "$100,000 to $124,999", "$125,000 to $149,999", "$150,000 to $199,999", 
    "$200,000 or more", "Total housing units", "Vacant housing units", 
    "Median gross rent", "Bachelor's degree holders", "Master's degree holders", 
    "Doctoral degree holders", "Unemployed population", "Employed population", 
    "Total working population", "Food stamp households", "Median family income", 
    "Total population", "Male population", "Female population", 
    "Total commuter population", "Total commuter population driving"
  )
)

# Retrieve ACS data
acs_data <- get_acs(
  geography = "tract",
  variables = census_variables$code,
  year = 2023,
  state = unique(full_data_customer$STATE),
  geometry = TRUE
)

# Merge with descriptions
acs_data <- acs_data %>%
  left_join(census_variables, by = c("variable" = "code"))

# Transform CRS to match customer data
data_sf <- st_transform(data_sf, st_crs(acs_data))

# Perform spatial join
joined_data_sf <- st_join(data_sf, acs_data, join = st_intersects)

# Reshape the dataset, keeping only the 'estimate' values
census <- joined_data_sf %>%
  mutate(
    variable_name = if_else(variable %in% census_variables$code, description, variable)
  ) %>%
  pivot_wider(
    names_from = variable_name,
    values_from = estimate,
    names_glue = "{variable_name}"
  )

# Select only the required columns
census <- census %>%
  dplyr::select(
    CUSTOMER_NUMBER, MED_HH_INC, GINI_IDX, PER_CAP_INC, MED_HOME_VAL, POV_POP, 
    INC_LVL_1, INC_LVL_2, INC_LVL_3, INC_LVL_4, INC_LVL_5, INC_LVL_6, 
    INC_LVL_7, INC_LVL_8, INC_LVL_9, INC_LVL_10, INC_LVL_11, INC_LVL_12, 
    INC_LVL_13, INC_LVL_14, INC_LVL_15, INC_LVL_16, TOT_HOUS_UNITS, 
    VAC_HOUS_UNITS, MED_GROSS_RENT, BACH_DEG, MAST_DEG, DOC_DEG, UNEMP_POP, 
    EMP_POP, TOT_WORK_POP, SNAP_HH, MED_FAM_INC, TOT_POP, MALE_POP, FEMALE_POP, 
    COMMUTE_POP, COMMUTE_POP_DRIVE
  )

# Remove the geometry column and convert to a normal data frame
census <- census %>%
  st_drop_geometry() %>% 
  as.data.frame()

# Handle missing and infinite values (replace -Inf with NA)
census[census == -Inf] <- NA

# Optionally impute missing values or remove them
census[is.na(census)] <- 0  # You could also choose to impute using other strategies

# Aggregate census data by CUSTOMER_NUMBER, keeping the highest value for each column
census <- census %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(across(everything(), max, na.rm = TRUE), .groups = "drop")

# Perform the join between full_data_customer and census on the CUSTOMER_NUMBER column
full_data_customer <- full_data_customer %>%
  dplyr::left_join(census, by = "CUSTOMER_NUMBER")

# Remove any duplicated columns or columns with ".x" suffixes
full_data_customer <- full_data_customer %>%
  dplyr::select(-ends_with(".x")) %>%
  dplyr::rename_with(~gsub("\\.y$", "", .), ends_with(".y"))

# Transforming variable types before save
full_data_customer$COLD_DRINK_CHANNEL <- as.factor(full_data_customer$COLD_DRINK_CHANNEL)
full_data_customer$TRADE_CHANNEL <- as.factor(full_data_customer$TRADE_CHANNEL)
full_data_customer$SUB_TRADE_CHANNEL <- as.factor(full_data_customer$SUB_TRADE_CHANNEL)
```


### 7.2 RMF Score

The RFM (Recency, Frequency, Monetary) analysis helps segment customers based on their purchasing behavior, allowing us to better understand consumption patterns. By adapting this model to analyze orders by customers, we aim to assess both frequency and volume of purchases. 

#### 7.2.1 Frequency - Days Between Orders

To adapt the RFM analysis considering purchase periods and quantities ordered, we will analyze the orders by customers. Before calculating the number of days between orders (frequency), we will establish the number of orders per customer across all transactions, considering only those where the order of gallons or cases is greater than 0.

```{r}
# Filter valid transactions (ORDERED_CASES > 0 or ORDERED_GALLONS > 0)
valid_orders <- full_data %>%
  filter(ORDERED_CASES > 0 | ORDERED_GALLONS > 0)

# Calculate the number of orders > 0 per customer
orders_per_customer <- valid_orders %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(NUM_ORDERS = n(), .groups = "drop") %>%
  ungroup()

# Add the column NUM_ORDERS in full_data_customer
full_data_customer <- full_data_customer %>%
  left_join(orders_per_customer, by = "CUSTOMER_NUMBER")

# Find customers who do not meet the condition (NO valid transactions)
customers_not_meeting_filter <- full_data_customer %>%
  filter(is.na(NUM_ORDERS)) %>%
  summarise(unique_customers = n_distinct(CUSTOMER_NUMBER))

# Print the number of unique customers who don't meet the filter
#print(customers_not_meeting_filter)

# Remove unnecessary intermediate data frames
rm(valid_orders, orders_per_customer,customers_not_meeting_filter)

```

There are 135 customers who do not have order transactions greater than zero in the dataset; for these customers, we will consider the number of delivery transactions as orders.

```{r}
# Filter customers with NUM_ORDERS == NA
customers_with_na_orders <- full_data_customer %>%
  filter(is.na(NUM_ORDERS)) %>%
  dplyr::select(CUSTOMER_NUMBER) %>%
  distinct()

# Filter valid delivery transactions (DELIVERED_CASES > 0 or DELIVERED_GALLONS > 0) in full_data
valid_deliveries <- full_data %>%
  filter(DELIVERED_CASES > 0 | DELIVERED_GALLONS > 0)

# Calculate the number of valid deliveries per customer with NUM_ORDERS == NA
deliveries_per_customer <- valid_deliveries %>%
  filter(CUSTOMER_NUMBER %in% customers_with_na_orders$CUSTOMER_NUMBER) %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(NUM_DELIVERIES = n()) %>%
  ungroup()

# Update NUM_ORDERS only for customers with NUM_ORDERS == NA
full_data_customer <- full_data_customer %>%
  left_join(deliveries_per_customer, by = "CUSTOMER_NUMBER") %>%
  mutate(
    NUM_ORDERS = if_else(is.na(NUM_ORDERS), NUM_DELIVERIES, NUM_ORDERS)
  ) %>%
  dplyr::select(-NUM_DELIVERIES)  # Drop the temporary NUM_DELIVERIES column

# Ensure full_data has the NUM_ORDERS column with the same values as full_data_customer
full_data <- full_data %>%
  left_join(full_data_customer %>% dplyr::select(CUSTOMER_NUMBER, NUM_ORDERS), by = "CUSTOMER_NUMBER")

# Remove unnecessary intermediate data frames
rm(customers_with_na_orders, valid_deliveries, deliveries_per_customer)

```

Considering all the order transactions recorded in 2023 and 2024, each unique customer has a minimum of 1 transaction and a maximum of 392 transactions.

To better understand the consumption profile of each customer, below we will visualize the number of customers in transaction bins where the orders of cases or gallons were greater than 0. For the 135 unique customers who did not have order transactions but received volume, we considered these operations as orders.


```{r}
# Count the number of valid transactions per customer
customers_by_bin <- full_data_customer %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(transaction_count = sum(NUM_ORDERS, na.rm = TRUE), .groups = "drop") %>%
  mutate(transaction_bin = case_when(
    transaction_count == 1 ~ "1",
    transaction_count >= 2 & transaction_count <= 10 ~ "2-10",
    transaction_count >= 11 & transaction_count <= 20 ~ "11-20",
    transaction_count >= 21 & transaction_count <= 30 ~ "21-30",
    transaction_count >= 31 & transaction_count <= 40 ~ "31-40",
    transaction_count >= 41 & transaction_count <= 50 ~ "41-50",
    transaction_count >= 51 & transaction_count <= 100 ~ "51-100",
    transaction_count >= 101 & transaction_count <= 200 ~ "101-200",
    transaction_count >= 201 & transaction_count <= 300 ~ "201-300",
    transaction_count > 300 ~ ">300",
    TRUE ~ "Other"
  )) %>%
  mutate(transaction_bin = factor(transaction_bin, levels = c("1", "2-10", "11-20", "21-30", "31-40", 
                                                             "41-50", "51-100", "101-200", "201-300", ">300"))) %>%
  group_by(transaction_bin) %>%
  summarise(unique_customers = n_distinct(CUSTOMER_NUMBER), .groups = "drop") %>%
  arrange(transaction_bin)

# Create a bar plot resembling a histogram of unique customers per transaction bin
ggplot(customers_by_bin, aes(x = transaction_bin, y = unique_customers, fill = transaction_bin)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = unique_customers), vjust = -0.3, size = 3, color = "black") +  # Add customer counts above bars
  scale_fill_brewer(palette = "Set3") +  # Use RColorBrewer's Set3 palette
  labs(title = "Number of Unique Customers by Transaction Count (Orders > 0)",
       x = "Transaction Count Bins",
       y = "Number of Unique Customers") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5, vjust = 0.5),  # Centered x-axis labels without rotation
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    axis.text = element_text(size = 9),  # Set the size of axis labels
    axis.title = element_text(size = 10)  # Set the size of axis titles
  )

# Remove unnecessary intermediate data frames
rm(customers_by_bin)

```



The histogram shows that 1,218 customers have only one order transaction, making it impossible to calculate the days between orders. Additionally, 6,798 customers have between 2 and 10 orders. To ensure more reliable figures, we will consider only customers with at least 11 orders for this indicator. As a result, all customers with fewer transactions will be assigned a value of 731 days between orders, indicating low order frequency over a two-year range.



```{r}
# Calculate the number of days between orders for customers with NUM_ORDERS >= 11
full_data <- full_data %>%
  arrange(CUSTOMER_NUMBER, TRANSACTION_DATE) %>%  # Sort by CUSTOMER_NUMBER and TRANSACTION_DATE
  group_by(CUSTOMER_NUMBER) %>%
  mutate(DAYS_BETWEEN_ORD = case_when(
    NUM_ORDERS <= 10 ~ 731,  # Set DAYS_BETWEEN_ORD to 731 for customers with NUM_ORDERS <= 10
    NUM_ORDERS >= 11 & 
      (ORDERED_CASES > 0 | ORDERED_GALLONS > 0) ~ 
      as.numeric(difftime(TRANSACTION_DATE, lag(TRANSACTION_DATE), units = "days")),  # Calculate days between orders for NUM_ORDERS >= 11 where ORDERED_CASES or ORDERED_GALLONS > 0
    NUM_ORDERS >= 11 & 
      !(ORDERED_CASES > 0 | ORDERED_GALLONS > 0) &  # Only apply this when the previous condition fails
      (DELIVERED_CASES > 0 | DELIVERED_GALLONS > 0) ~ 
      as.numeric(difftime(TRANSACTION_DATE, lag(TRANSACTION_DATE), units = "days")),  # If no ORDERED_CASES or ORDERED_GALLONS > 0, calculate with DELIVERED_CASES or DELIVERED_GALLONS
    TRUE ~ NA_real_  # For all other cases
  )) %>%
  ungroup()


# Calculate the average days between orders per customer and round the result to the nearest integer
avg_days_per_customer <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(AVG_DAYS_BET_ORD = round(mean(DAYS_BETWEEN_ORD, na.rm = TRUE), 0)) %>%  # Round to nearest integer
  ungroup()

# Update full_data_customer with the average days between orders
full_data_customer <- full_data_customer %>%
  left_join(avg_days_per_customer, by = "CUSTOMER_NUMBER")

# Remove temporary variables
rm(avg_days_per_customer)


```

```{r}
# Count the number of unique customers in each days between orders bin without adding a new column to the dataset
customers_by_bin <- full_data_customer %>%
  mutate(DAYS_BETWEEN_ORD_BIN = case_when(
    AVG_DAYS_BET_ORD >= 1 & AVG_DAYS_BET_ORD <= 10 ~ "1-10 days",
    AVG_DAYS_BET_ORD > 10 & AVG_DAYS_BET_ORD <= 20 ~ "11-20 days",
    AVG_DAYS_BET_ORD > 20 & AVG_DAYS_BET_ORD <= 30 ~ "21-30 days",
    AVG_DAYS_BET_ORD > 30 & AVG_DAYS_BET_ORD <= 40 ~ "31-40 days",
    AVG_DAYS_BET_ORD > 40 & AVG_DAYS_BET_ORD <= 50 ~ "41-50 days",
    AVG_DAYS_BET_ORD > 50 ~ "Above 50 days",
    TRUE ~ "NA"
  )) %>%
  group_by(DAYS_BETWEEN_ORD_BIN) %>%
  summarise(unique_customers = n_distinct(CUSTOMER_NUMBER), .groups = "drop") %>%
  mutate(percentage_customers = unique_customers / sum(unique_customers) * 100) %>%  # Calculate percentage
  arrange(DAYS_BETWEEN_ORD_BIN)

# Create a bar plot resembling a histogram of unique customers percentage per days between orders bin
ggplot(customers_by_bin, aes(x = DAYS_BETWEEN_ORD_BIN, y = percentage_customers, fill = DAYS_BETWEEN_ORD_BIN)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = scales::percent(percentage_customers / 100)), vjust = -0.3, size = 3) +  # Add percentage labels above bars
  scale_fill_brewer(palette = "Set3") +  # Use RColorBrewer's Set3 palette
  labs(title = "Percentage of Unique Customers by Days Between Orders",
       x = "Days Between Orders",
       y = "Percentage of Unique Customers") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5, vjust = 0.5),  # Centered x-axis labels without rotation
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    axis.text = element_text(size = 9),  # Set the size of axis labels
    axis.title = element_text(size = 10)  # Set the size of axis titles
  )

# Remove unnecessary intermediate data frames
rm(customers_by_bin)

```

Around 20% of customers have an average order interval of up to 10 days, while 44% have an average interval of more than 30 days.

#### 7.2.2  Recency - Time Since Last Order

To calculate recency, we will consider the number of days between the date of the last order and 01-01-2025.

```{r}
# Create the LAST_ORDER_DATE column, excluding rows where all specified columns are zero
full_data <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  mutate(
    LAST_ORDER_DATE = if_else(
      (ORDERED_CASES > 0 | ORDERED_GALLONS > 0) & 
      !(ORDERED_CASES == 0 & ORDERED_GALLONS == 0 & LOADED_CASES == 0 & LOADED_GALLONS == 0 & DELIVERED_CASES == 0 & DELIVERED_GALLONS == 0),
      as.character(max(TRANSACTION_DATE, na.rm = TRUE)), 
      NA_character_
    )
  ) %>%
  ungroup()

```

There are 5,754 transaction rows where it's not possible to assign the last transaction date based on orders. For these, we will use the date of the last delivery operation as the reference date. The last two transactions, which refer to return transactions, will be excluded.


```{r, message=FALSE}
# For customers with LAST_ORDER_DATE as NA, consider the latest TRANSACTION_DATE where DELIVERED_CASES or DELIVERED_GALLONS > 0
full_data <- full_data %>%
  mutate(LAST_ORDER_DATE = as.Date(LAST_ORDER_DATE)) %>%  # Convert LAST_ORDER_DATE to Date format
  group_by(CUSTOMER_NUMBER) %>%
  mutate(
    LAST_ORDER_DATE = if_else(
      is.na(LAST_ORDER_DATE) & (ORDERED_CASES == 0 & ORDERED_GALLONS == 0),
      as.Date(max(TRANSACTION_DATE[DELIVERED_CASES > 0 | DELIVERED_GALLONS > 0], na.rm = TRUE)),
      LAST_ORDER_DATE
    )
  ) %>%
  ungroup()


# Remove the last 2 rows where LAST_ORDER_DATE is NA (return operations only)
full_data <- full_data %>%
  filter(!is.na(LAST_ORDER_DATE))

# Remove rows where LAST_ORDER_DATE is Inf (return operations only)
full_data <- full_data %>%
  filter(!is.infinite(LAST_ORDER_DATE))

# Reference Date
reference_date <- as.Date("2025-01-01")

# Create the DAYS_AF_LAST_ORD column in full_data
full_data <- full_data %>%
  mutate(
    DAYS_AF_LAST_ORD = ifelse(!is.na(LAST_ORDER_DATE), 
                              as.numeric(difftime(reference_date, LAST_ORDER_DATE, units = "days")),
                              NA_real_))

```

```{r}
# Aggregate full_data to get the latest LAST_ORDER_DATE and DAYS_AF_LAST_ORD for each CUSTOMER_NUMBER
full_data_aggregated <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(
    LAST_ORDER_DATE = max(LAST_ORDER_DATE, na.rm = TRUE),
    DAYS_AF_LAST_ORD = max(DAYS_AF_LAST_ORD, na.rm = TRUE),
    .groups = 'drop'
  )

# Join the aggregated data with full_data_customer
full_data_customer <- full_data_customer %>%
  left_join(full_data_aggregated, by = "CUSTOMER_NUMBER")


# # Remove unnecessary intermediate data frames
rm(full_data_aggregated)
```



#### 7.2.3 Total Quantity Ordered

Since we do not have access to the prices charged, and understanding that these are likely different among customer types and demanded volumes, instead of considering monetary values, we will focus on the quantities demanded. This is because our current focus is on customer segmentation.

```{r}
# Calculate the total ordered by customer by summing ORDERED_CASES and ORDERED_GALLONS
total_ordered_per_customer <- full_data %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(TOTAL_ORDERED = sum(ORDERED_CASES, na.rm = TRUE) + sum(ORDERED_GALLONS, na.rm = TRUE)) %>%
  ungroup()

# Add the TOTAL_ORDERED column to full_data_customer by CUSTOMER_NUMBER
full_data_customer <- full_data_customer %>%
  left_join(total_ordered_per_customer, by = "CUSTOMER_NUMBER")

# Identify customers with TOTAL_ORDERED == 0
customers_with_zero_ordered <- total_ordered_per_customer %>%
  filter(TOTAL_ORDERED == 0)

# For those customers, calculate DELIVERED_CASES + DELIVERED_GALLONS from full_data
deliveries_for_zero_orders <- full_data %>%
  filter(CUSTOMER_NUMBER %in% customers_with_zero_ordered$CUSTOMER_NUMBER) %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarise(DELIVERED_TOTAL = sum(DELIVERED_CASES, na.rm = TRUE) + sum(DELIVERED_GALLONS, na.rm = TRUE)) %>%
  ungroup()

# Merge the delivery values into the total_ordered_per_customer dataframe,
# ensuring that if TOTAL_ORDERED is zero, it is replaced by DELIVERED_TOTAL
total_ordered_per_customer <- total_ordered_per_customer %>%
  left_join(deliveries_for_zero_orders, by = "CUSTOMER_NUMBER") %>%
  mutate(
    TOTAL_ORDERED = if_else(TOTAL_ORDERED == 0, DELIVERED_TOTAL, TOTAL_ORDERED)
  ) %>%
  dplyr::select(CUSTOMER_NUMBER, TOTAL_ORDERED)

# Add the updated TOTAL_ORDERED column to full_data_customer by CUSTOMER_NUMBER
full_data_customer <- full_data_customer %>%
  left_join(total_ordered_per_customer, by = "CUSTOMER_NUMBER")

# Remove the 'TOTAL_ORDERED.x' column and rename 'TOTAL_ORDERED.y' to 'TOTAL_ORDERED'
full_data_customer <- full_data_customer %>%
  dplyr::select(-TOTAL_ORDERED.x) %>%
  dplyr::rename(TOTAL_ORDERED = TOTAL_ORDERED.y)

# Remove unnecessary intermediate data frames
rm(total_ordered_per_customer, customers_with_zero_ordered, deliveries_for_zero_orders)

```



#### 7.2.4 Adapted RFM Score

Using the created variables, we will assign scores to classes based on their distribution. The total score, along with its relative weight, forms the *RFM_SCORE*, which serves as an additional variable for customer analysis and segmentation.

We use the quantitative distribution of the variables to assign scores, as some have a wide range. Each variable will receive a score between 1 and 10. For frequency, we created two variables and decided to give weight not only to the number of orders but also to the interval between them. As a result, the minimum score is 4, and the maximum is 40.

```{r}
# Remove previously created columns
#full_data_customer <- full_data_customer %>%
#  dplyr::select(-FREQUENCY_SCORE, -RECENCY_SCORE, -VOLUME_SCORE, -RFM_SCORE)

# Create Frequency Score based on NUM_ORDERS
full_data_customer <- full_data_customer %>%
  mutate(
    ORDER_FREQUENCY_SCORE = case_when(
      NUM_ORDERS >= 300 ~ 10,  
      NUM_ORDERS >= 200 ~ 9,
      NUM_ORDERS >= 150 ~ 8,
      NUM_ORDERS >= 100 ~ 7,
      NUM_ORDERS >= 75  ~ 6,
      NUM_ORDERS >= 50  ~ 5,  # 3rd quartile
      NUM_ORDERS >= 35  ~ 4,  # Mean
      NUM_ORDERS >= 23  ~ 3,  # Median
      NUM_ORDERS >= 10  ~ 2,  # 1st quartile
      TRUE ~ 1  
    ),
    ORDER_INTERVAL_SCORE = case_when(
      AVG_DAYS_BET_ORD <= 5   ~ 10,
      AVG_DAYS_BET_ORD <= 13  ~ 9, # 1st quartile
      AVG_DAYS_BET_ORD <= 20  ~ 8,
      AVG_DAYS_BET_ORD <= 26  ~ 7, # Median
      AVG_DAYS_BET_ORD <= 30  ~ 6, 
      AVG_DAYS_BET_ORD <= 50  ~ 5,  
      AVG_DAYS_BET_ORD <= 100 ~ 4,
      AVG_DAYS_BET_ORD <= 210 ~ 3, # Mean
      AVG_DAYS_BET_ORD <= 300 ~ 2,
      TRUE ~ 1  
    )
  )

# Create Recency Score based on DAYS_AF_LAST_ORD
full_data_customer <- full_data_customer %>%
  mutate(
    RECENCY_SCORE = case_when(
      DAYS_AF_LAST_ORD <= 7   ~ 10,  
      DAYS_AF_LAST_ORD <= 13  ~ 9,  # 1st quartile
      DAYS_AF_LAST_ORD <= 20  ~ 8,  
      DAYS_AF_LAST_ORD <= 27  ~ 7,  #Median
      DAYS_AF_LAST_ORD <= 40  ~ 6,  
      DAYS_AF_LAST_ORD <= 50  ~ 5,
      DAYS_AF_LAST_ORD <= 72  ~ 4,  #Mean
      DAYS_AF_LAST_ORD <= 90  ~ 3,  #3rd quartile
      DAYS_AF_LAST_ORD <= 180 ~ 2,  #Six months
      TRUE ~ 1  
    )
  )

# Create Volume Score based on TOTAL_ORDERED
full_data_customer <- full_data_customer %>%
  mutate(
    VOLUME_SCORE = case_when(
      TOTAL_ORDERED >= 300000 ~ 10,  
      TOTAL_ORDERED >= 100000 ~ 9,
      TOTAL_ORDERED >= 5000   ~ 8,
      TOTAL_ORDERED >= 2000   ~ 7,
      TOTAL_ORDERED >= 1267   ~ 6,  # Mean 
      TOTAL_ORDERED >= 815    ~ 5,  # 3rd quartile
      TOTAL_ORDERED >= 400    ~ 4,  # Threshold
      TOTAL_ORDERED >= 302    ~ 3,  # Median
      TOTAL_ORDERED >= 200    ~ 2,  
      TRUE ~ 1  
    )
  )


```

```{r}

# Calculate the overall RFM Score as the sum of Recency, Frequency, Order Interval, and Volume scores
full_data_customer <- full_data_customer %>%
  mutate(
    RFM_SCORE = RECENCY_SCORE + ORDER_FREQUENCY_SCORE + ORDER_INTERVAL_SCORE + VOLUME_SCORE
  )


# Count the number of customers in each RFM_SCORE range
rfm_distribution <- full_data_customer %>%
  mutate(RFM_CATEGORY = case_when(
    RFM_SCORE <= 10 ~ "4-10",
    RFM_SCORE <= 20 ~ "11-20",
    RFM_SCORE <= 30 ~ "21-30",
    TRUE ~ "31-40"
  )) %>%
  group_by(RFM_CATEGORY) %>%
  summarise(CUSTOMER_COUNT = n(), .groups = "drop") %>%
  mutate(PERCENTAGE = CUSTOMER_COUNT / sum(CUSTOMER_COUNT) * 100)

# Reorder RFM_CATEGORY to ensure it starts with scores between 4 and 10
rfm_distribution$RFM_CATEGORY <- factor(rfm_distribution$RFM_CATEGORY, 
                                        levels = c("4-10", "11-20", "21-30", "31-40"))

# Plot the distribution of RFM scores
ggplot(rfm_distribution, aes(x = RFM_CATEGORY, y = PERCENTAGE, fill = RFM_CATEGORY)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = paste0(round(PERCENTAGE, 1), "%")), vjust = -0.3, size = 4) +
  scale_fill_brewer(palette = "Set3") +  # Use Set3 color palette
  labs(title = "Distribution of Customers by RFM Score",
       x = "RFM Score Range",
       y = "Percentage of Customers") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11)
  )

# Remove unnecessary intermediate data frame
rm(rfm_distribution)


```

The adapted RFM Score was a method we developed to condense various pieces of information related to store consumption. We found that 60% of stores have a score up to 20 (which is the median), 32% have scores between 21-30, and 8.5% have scores above 30. This indicates that there is a small number of stores with high consumption patterns.


```{r}
# Filter only customers where LOCAL_FOUNT_ONLY == 1
rfm_distribution_lfo <- full_data_customer %>%
  filter(LOCAL_FOUNT_ONLY == 1) %>%
  mutate(RFM_CATEGORY = case_when(
    RFM_SCORE <= 10 ~ "4-10",
    RFM_SCORE <= 20 ~ "11-20",
    RFM_SCORE <= 30 ~ "21-30",
    TRUE ~ "31-40"
  )) %>%
  group_by(RFM_CATEGORY) %>%
  summarise(CUSTOMER_COUNT = n(), .groups = "drop") %>%
  mutate(PERCENTAGE = CUSTOMER_COUNT / sum(CUSTOMER_COUNT) * 100)

# Reorder RFM_CATEGORY to ensure it starts with scores between 4 and 10
rfm_distribution_lfo$RFM_CATEGORY <- factor(rfm_distribution_lfo$RFM_CATEGORY, 
                                            levels = c("4-10", "11-20", "21-30", "31-40"))

# Plot the distribution of RFM scores for LOCAL_FOUNT_ONLY == 1
ggplot(rfm_distribution_lfo, aes(x = RFM_CATEGORY, y = PERCENTAGE, fill = RFM_CATEGORY)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = paste0(round(PERCENTAGE, 1), "%")), vjust = -0.3, size = 4) +
  scale_fill_brewer(palette = "Set3") +  # Use Set3 color palette
  labs(title = "Distribution of Customers by RFM Score (LOCAL_FOUNT_ONLY = 1)",
       x = "RFM Score Range",
       y = "Percentage of Customers") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11)
  )

# Remove unnecessary intermediate data frame
rm(rfm_distribution_lfo)


```

For customers who are local partners and consume only fountain drinks, it is clear that their consumption patterns are even lower. Nearly 74% of them have scores up to 20, and among the remaining customers, less than 3.6% have scores above 30.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Save full_data as CSV
#write.csv(full_data, file = "full_data.csv", row.names = FALSE)

# Save full_data_customer as CSV
#write.csv(full_data_customer, file = "full_data_customer.csv", row.names = FALSE)

# Load the full_data CSV
#full_data <- read.csv(file = "full_data.csv", sep = ",", stringsAsFactors = FALSE)

# Load the full_data_customer CSV
#full_data_customer <- read.csv(file = "full_data_customer.csv", sep = ",", stringsAsFactors = FALSE)

```

### 7.3 Customer Demand and Growth

#### 7.3.1 Low Demand Customers

We know that there are few customers with very high consumption volumes, which skews the average well above the median. The table below explores metrics related to customers whose demand is below the first quartile.

```{r}
# Summarize the metrics
data_summary <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(
    Avg_Vol_Cust = round(mean((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024), na.rm = TRUE)),
    Median_Vol_Cust = round(median((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024), na.rm = TRUE)),
    First_Quartile_Vol = round(quantile((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024), 0.25, na.rm = TRUE)),
    .groups = 'drop'
  )

# Calculate the first quartile for each channel
quartile_data <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(
    First_Quartile_Val = round(quantile((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024), 0.25, na.rm = TRUE)),
    Tot_Vol = sum((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024), na.rm = TRUE),
    .groups = 'drop'
  )

# Calculate the number of customers below the first quartile and their total volume
below_quartile_stats <- full_data_customer %>%
  left_join(quartile_data %>% dplyr::select(COLD_DRINK_CHANNEL, First_Quartile_Val), by = "COLD_DRINK_CHANNEL") %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(
    Num_Customers_Below_1Q = sum(((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024)) <= First_Quartile_Val, na.rm = TRUE),
    Vol_Below_1Q = sum(((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024))[((QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024)) <= First_Quartile_Val], na.rm = TRUE),
    .groups = 'drop'
  )

# Count the total number of customers per channel
customer_count <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(N_Cust = n(), .groups = 'drop')

# Combine all the data
final_summary <- customer_count %>%
  left_join(data_summary, by = "COLD_DRINK_CHANNEL") %>%
  left_join(quartile_data, by = "COLD_DRINK_CHANNEL") %>%
  left_join(below_quartile_stats, by = "COLD_DRINK_CHANNEL") %>%
  mutate(
    Vol_Perct = round((Vol_Below_1Q / Tot_Vol) * 100, 1),
    First_Quartile_Vol = as.integer(First_Quartile_Val)
  ) %>%
  dplyr::select(COLD_DRINK_CHANNEL, N_Cust, Avg_Vol_Cust, Median_Vol_Cust, First_Quartile_Vol, 
                Num_Customers_Below_1Q, Vol_Perct)

# Display the table with kable and styling
kable(final_summary, format = "html", escape = FALSE, align = "c", 
     col.names = c("Cold Drink Channel", "Total Cust.", "Avg. Vol Cust.", "Median Vol Cust.", 
                    "1st Quartile Qtd", "Cust. Below 1st Quart", "Vol % Below 1st Quart")) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:7, width = "6em") %>%
  row_spec(0, bold = TRUE, color = "black", background = "lightyellow") %>%
  add_header_above(c("Customers Analysis by Cold Drink Channel" = 7)) %>%
  kable_paper("striped", full_width = FALSE)

```

For customers whose total consumption volumes in 2023 and 2024 are below the first quartile, it is evident that the sums of their volumes represent very low percentages, ranging from 0.3% to 3.1% of the total for each segment. Notably, in the dining segment, 25% of customers showed demand below the first quartile.

Some of these customers have been classified as high growth potential and channel high growth potential due to their demand growth being above average. This is likely because any increase in demand from low-volume customers results in higher growth percentages.

The low RFM scores also indicate that these customers have low recency, frequency, and total volume. Therefore, we will create a flag, **LOW_DEMAND_CUST**, where a value of 1 will indicate low-consumption customers. With this flag, we aim to assign a white truck to these customers, regardless of their growth indices.

Below are the cut volumes by segment:
```{r}
# Extract the list of 'Cold Drink Channel' and '1st Quartile Qty'
list_summary <- final_summary %>%
  dplyr::select(COLD_DRINK_CHANNEL, First_Quartile_Vol) %>%
  deframe()

# Display the list
list_summary

# Calculate the sum per row and assign LOW_DEMAND_CUST
full_data_customer <- full_data_customer %>%
  mutate(
    Total_Vol_Cust = (QTD_DLV_CA_2023 + QTD_DLV_CA_2024) + (QTD_DLV_GAL_2023 + QTD_DLV_GAL_2024),
    LOW_DEMAND_CUST = if_else(Total_Vol_Cust <= list_summary[COLD_DRINK_CHANNEL], 1, 0)
  )

```

In the plot below, the numbers represent the percentages and the number of customers who received this flag.

```{r}
# Group and calculate the number of customers with LOW_DEMAND_CUST by LOCAL_FOUNT_ONLY
summary_low_demand <- full_data_customer %>%
  group_by(LOCAL_FOUNT_ONLY, LOW_DEMAND_CUST) %>%
  summarise(
    total_customers = n(),
    .groups = "drop"
  )

# Calculate the percentage for each group
summary_low_demand <- summary_low_demand %>%
  group_by(LOCAL_FOUNT_ONLY) %>%
  mutate(
    percentage = total_customers / sum(total_customers) * 100
  )

# Plot for percentages with LOW_DEMAND_CUST as fill and LOCAL_FOUNT_ONLY as groups
ggplot(summary_low_demand, aes(x = factor(LOCAL_FOUNT_ONLY), y = percentage, fill = factor(LOW_DEMAND_CUST))) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.6) +
  geom_text(aes(label = paste0(scales::comma(percentage, suffix = "%"), " (", total_customers, ")")),
            position = position_dodge(width = 0.8), vjust = -0.2, size = 3.5) +
  labs(title = "Percentage of Customers with Low Demand") +
  scale_fill_manual(values = c("0" = "darkolivegreen", "1" = "sandybrown"), 
                    labels = c("0" = "Others (Above Q1)", "1" = "Low Demand")) +  # Set colors and labels for LOW_DEMAND_CUST
  theme_minimal() +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.title = element_text(face = "bold", size = 10),  # Add legend title
    legend.position = "right",  # Position legend on the right side
    legend.box = "vertical",  # Ensure vertical arrangement for the legend
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.text = element_text(size = 10, face = "bold"),
    strip.background = element_blank(),
    axis.text.x = element_text(size = 10, angle = 0),  # Display x-axis labels without rotation
    panel.spacing = unit(1, "lines"),
    strip.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  scale_x_discrete(labels = c("0" = "Others", "1" = "Local Fountain Only")) +  # Set x-axis labels
  guides(fill = guide_legend(title = "Low Demand Status"))  # Add legend title


```


#### 7.3.2 Demand Variation Among All Stores

To measure demand growth patterns across our customer base (January 2023 - December 2024):

1. **Data Preparation:** Combined monthly case and gallon deliveries for each customer into total monthly volumes.

2. **Eligibility:** Required ≥6 months with positive orders for reliable analysis. Customers with <6 ordering months were classified as having no growth potential (6,026 customers).

3. **Growth Calculation:**
   - Split each qualifying customer's order history into two equal time periods
   - For odd numbers of months, divided the middle month equally between periods
   - Calculated growth rate as: (Second Period Total - First Period Total) / First Period Total

4. **Classification:** Customers with growth rates exceeding the average positive growth rate were categorized as high growth potential (HIGH_GROW_POT = 1), while all others received a standard classification (HIGH_GROW_POT = 0).


```{r}
# Initialize new columns in the dataset
full_data_customer$NUM_POSITIVE_SUMS <- 0
full_data_customer$QTD_DLV_FIRST_HALF <- 0
full_data_customer$QTD_DLV_SECOND_HALF <- 0
full_data_customer$DEMAND_VARIATION <- NA  # Initialize as NA

# Process each customer individually
for (i in 1:nrow(full_data_customer)) {
  # Create a vector of positive sums while maintaining the chronological order
  POSITIVE_SUMS <- c()
  
  # Iterate over the 24 months in the correct sequence
  for (j in 1:24) {
    # Create column names
    year <- 2023 + (j - 1) %/% 12
    month <- (j - 1) %% 12 + 1
    
    CA_COL <- paste0("QTD_DLV_CA_", sprintf("%04d", year), "_", sprintf("%02d", month))
    GAL_COL <- paste0("QTD_DLV_GAL_", sprintf("%04d", year), "_", sprintf("%02d", month))
    
    # Check if columns exist in the dataset
    if (CA_COL %in% names(full_data_customer) && GAL_COL %in% names(full_data_customer)) {
      CA_VALUE <- full_data_customer[[CA_COL]][i]
      GAL_VALUE <- full_data_customer[[GAL_COL]][i]
      
      # Replace NA with 0
      CA_VALUE <- ifelse(is.na(CA_VALUE), 0, CA_VALUE)
      GAL_VALUE <- ifelse(is.na(GAL_VALUE), 0, GAL_VALUE)
      
      # Sum values for the month
      SUM_VALUE <- CA_VALUE + GAL_VALUE
      
      # Add to the list if positive
      if (SUM_VALUE > 0) {
        POSITIVE_SUMS <- c(POSITIVE_SUMS, SUM_VALUE)
      }
    }
  }
  
  # Total number of positive operations
  num_operations <- length(POSITIVE_SUMS)
  full_data_customer$NUM_POSITIVE_SUMS[i] <- num_operations
  
  # If fewer than 6 positive sums, set values accordingly and continue
  if (num_operations < 6) {
    full_data_customer$QTD_DLV_FIRST_HALF[i] <- 0
    full_data_customer$QTD_DLV_SECOND_HALF[i] <- 0
    full_data_customer$DEMAND_VARIATION[i] <- NA
    next
  }
  
  # Initialize the two halves
  QTD_DLV_FIRST_HALF <- 0
  QTD_DLV_SECOND_HALF <- 0
  
  # Split the operations into two halves
  if (num_operations %% 2 == 0) {
    # If even number of operations
    mid_point <- num_operations / 2
    QTD_DLV_FIRST_HALF <- sum(POSITIVE_SUMS[1:mid_point])
    QTD_DLV_SECOND_HALF <- sum(POSITIVE_SUMS[(mid_point + 1):num_operations])
  } else {
    # If odd number of operations
    mid_point <- (num_operations + 1) %/% 2
    
    # Split the middle value between both halves
    first_part <- if(mid_point > 1) POSITIVE_SUMS[1:(mid_point - 1)] else numeric(0)
    central_value <- POSITIVE_SUMS[mid_point] / 2
    second_part <- if(mid_point < num_operations) POSITIVE_SUMS[(mid_point + 1):num_operations] else numeric(0)
    
    QTD_DLV_FIRST_HALF <- sum(c(first_part, central_value))
    QTD_DLV_SECOND_HALF <- sum(c(central_value, second_part))
  }
  
  # Assign values to the dataset
  full_data_customer$QTD_DLV_FIRST_HALF[i] <- QTD_DLV_FIRST_HALF
  full_data_customer$QTD_DLV_SECOND_HALF[i] <- QTD_DLV_SECOND_HALF
  
  # Calculate demand variation
  if (QTD_DLV_FIRST_HALF > 0) {  # Avoid division by zero
    DEMAND_VARIATION_VALUE <- (QTD_DLV_SECOND_HALF - QTD_DLV_FIRST_HALF) / QTD_DLV_FIRST_HALF
    full_data_customer$DEMAND_VARIATION[i] <- DEMAND_VARIATION_VALUE
  } else {
    full_data_customer$DEMAND_VARIATION[i] <- NA
  }
}

# Create the HIGH_GROW_POT column
full_data_customer$HIGH_GROW_POT <- 0  # Initialize all values to 0

# Calculate the mean of DEMAND_VARIATION for positive values only
positive_variations <- full_data_customer$DEMAND_VARIATION[full_data_customer$DEMAND_VARIATION > 0]
if (length(positive_variations) > 0) {
  mean_value <- mean(positive_variations, na.rm = TRUE)
  
  # Display the calculated mean
  cat("Calculated mean of positive DEMAND_VARIATION: ", mean_value, "\n")
  
  # Assign 1 for customers with DEMAND_VARIATION greater than the mean
  full_data_customer$HIGH_GROW_POT <- ifelse(!is.na(full_data_customer$DEMAND_VARIATION) & 
                                            full_data_customer$DEMAND_VARIATION > mean_value, 
                                            1, 
                                            full_data_customer$HIGH_GROW_POT)
} else {
  cat("No positive DEMAND_VARIATION values found\n")
}
```
Considering all customers, there was an average demand growth variation of 28%. However, 6,026 customers were excluded from the analysis as their growth could not be calculated due to having fewer than 6 periods of orders. For these customers, it was assumed that they have no growth potential.

Below, we can see the number of customers whose growth exceeded the average, regardless of the segment.


```{r}
# Group and calculate the percentage of customers with HIGH_GROW_POT = 1 and 0 by LFO
summary_high_growth <- full_data_customer %>%
  group_by(LOCAL_FOUNT_ONLY) %>%
  summarise(
    high_growth = sum(HIGH_GROW_POT == 1, na.rm = TRUE),
    low_growth = sum(HIGH_GROW_POT == 0, na.rm = TRUE),
    total_customers = n(),
    .groups = "drop"
  ) %>%
  mutate(
    pct_high_growth = high_growth / total_customers * 100,
    pct_low_growth = low_growth / total_customers * 100
  )

# Transform data into long format for percentages
summary_high_growth_long <- summary_high_growth %>%
  pivot_longer(
    cols = starts_with("pct_"),
    names_to = "growth_type",
    values_to = "percentage"
  ) %>%
  mutate(
    growth_type = factor(growth_type, 
                         levels = c("pct_low_growth", "pct_high_growth"),  # Invertendo a ordem
                         labels = c("Low Growth Potential", "High Growth Potential"))
  )

# Ensure LFO is a factor
summary_high_growth_long$LOCAL_FOUNT_ONLY <- factor(summary_high_growth_long$LOCAL_FOUNT_ONLY, levels = c("0", "1"))

# Plot for percentages with the legend on the side
ggplot(summary_high_growth_long, aes(x = LOCAL_FOUNT_ONLY, y = percentage, fill = growth_type)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.6) +
  geom_text(aes(label = scales::comma(percentage, suffix = "%")), 
            position = position_dodge(width = 0.8), vjust = 0.2, size = 3.5) +
  labs(title = "Percentage of Customers Classified as Low or High Growth Potential") +
  scale_fill_manual(values = c("Low Growth Potential" = "#FF6347", "High Growth Potential" = "#40E0D0")) +  # Invertendo cores
  theme_minimal() +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.title = element_blank(),  # Remove legend title
    legend.position = "right",  # Position legend on the right side
    legend.box = "vertical",  # Ensure vertical arrangement for the legend
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.text = element_text(size = 10, face = "bold"),
    strip.background = element_blank(),
    axis.text.x = element_text(size = 10),
    panel.spacing = unit(1, "lines"),
    strip.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  scale_x_discrete(labels = c("0" = "Others", "1" = "Local Fountain Only")) +
  guides(fill = guide_legend(title = "Growth Potential"))  # Add a legend title

# Group and calculate the number of customers with HIGH_GROW_POT = 1 and 0 by LFO
summary_high_growth <- full_data_customer %>%
  group_by(LOCAL_FOUNT_ONLY) %>%
  summarise(
    low_growth = sum(HIGH_GROW_POT == 0, na.rm = TRUE),  # Invertendo a ordem das colunas
    high_growth = sum(HIGH_GROW_POT == 1, na.rm = TRUE),
    .groups = "drop"
  )

# Display the summary with the count of customers
#summary_high_growth


```

Approximately 9% of customers (123) identified as local market partners who purchase fountain-only products show high growth potential according to the established criteria. For other customers, about 12% (3450) are classified as having high growth potential.

We acknowledge that customers with high volumes are somewhat penalized by this criterion, as it is challenging for them to achieve significant demand growth. However, their substantial volume already positions them as strategic partners, essential for close monitoring and receiving deliveries via red trucks. For these customers, the distribution cost is lower, which makes more competitive pricing feasible, ensuring the sustainability of the partnership.

#### 7.3.3 Demand Variation by Cold Drink Channel

First, we will recall the weight of each cold drink channel in the total consumption of cases and gallons.

```{r}
# Define the custom color palette for COLD_DRINK_CHANNEL
custom_palette <- c(
  "#F4EBE8", "#8ED081", "#ABD2FA", "#A7ADC6", "#B33951",  
  "#FFD700", "#FF6347", "#20B2AA", "#87CEEB", "#D3D3D3", "#FF8C00",   
  "#32CD32", "#6A5ACD", "#FF1493", "#40E0D0", "#FF4500", "#D2691E"
)

# Summarize data by COLD_DRINK_CHANNEL, summing the quantities of gallons and cases
data_summary <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(
    Total_Volume = sum(QTD_DLV_GAL_2023, na.rm = TRUE) + sum(QTD_DLV_GAL_2024, na.rm = TRUE) + 
                   sum(QTD_DLV_CA_2023, na.rm = TRUE) + sum(QTD_DLV_CA_2024, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  mutate(Percentage = round(Total_Volume / sum(Total_Volume) * 100, 1))  # Calculate the percentage

# Create a horizontal bar chart for the percentage of total volume by cold drink channel
ggplot(data_summary, aes(x = Percentage, y = reorder(COLD_DRINK_CHANNEL, Percentage), fill = COLD_DRINK_CHANNEL)) +
  geom_bar(stat = "identity", position = "stack", alpha = 0.7) +  
  geom_text(aes(label = paste(Percentage, "%")), position = position_stack(vjust = 0.5), 
            hjust = -0.01, color = "black", size = 3.2) +
  labs(title = "Percentage of Total Volume by Cold Drink Channel",
       x = "Percentage of Total Volume", 
       y = NULL) +  
  scale_x_continuous(labels = function(x) paste0(x, "%")) +  # Format x-axis as percentages
  scale_fill_manual(values = custom_palette) +  # Apply the custom color palette
  theme_minimal() +  
  theme(
    plot.title = element_text(size = 12, face = "bold"),  
    axis.text.y = element_text(size = 10),  
    axis.title.x = element_blank(),  # Remove the x-axis title
    axis.text.x = element_text(size = 10),  # Add x-axis text size
    legend.position = "none",  # Remove the legend
    panel.grid.major.x = element_line(color = "grey90"),  # Add only vertical grid lines
    panel.grid.major.y = element_blank(),  # Remove horizontal grid lines
    panel.grid.minor = element_blank()  # Remove minor grid lines
  )

```

We decided to consider each customer's growth potential within their segment. Following the same criteria as before, we classify as high potential only those customers whose demand variation exceeds the average of their respective segments.  
Below is the calculated demand variation for each Cold Drink Channel during the period.

```{r}
# Define the custom color palette for COLD_DRINK_CHANNEL with unique colors
custom_palette <- c(
  "#F4EBE8", "#8ED081", "#ABD2FA", "#A7ADC6", "#B33951",  
  "#FFD700", "#FF6347", "#20B2AA", "#87CEEB", "#D3D3D3", "#FF8C00",   
  "#32CD32", "#6A5ACD", "#FF1493", "#40E0D0", "#FF4500", "#D2691E"
)

# Aggregate the data to calculate the mean DEMAND_VARIATION by COLD_DRINK_CHANNEL
summary_growth_channel <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(CHANNEL_VAR = mean(DEMAND_VARIATION, na.rm = TRUE))  # Mean DEMAND_VARIATION per channel

# Create the horizontal bar chart for CHANNEL_VAR by COLD_DRINK_CHANNEL
ggplot(summary_growth_channel, aes(x = CHANNEL_VAR, y = reorder(COLD_DRINK_CHANNEL, CHANNEL_VAR), fill = COLD_DRINK_CHANNEL)) +
  geom_bar(stat = "identity", position = "stack", alpha = 0.6) +
  geom_text(aes(label = paste0(round(CHANNEL_VAR * 100, 1), "%")),  # Round labels to 1 decimal place
            position = position_stack(vjust = 0.5), 
            hjust = -0.01, 
            color = "black", size = 3.2) +
  labs(title = "Average Demand Variation by Cold Drink Channel",
       x = "Percentage Variation (%)", 
       y = NULL) +  
  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1), expand = expansion(c(0, 0.05))) +  # Limit to 1 decimal place
  scale_fill_manual(values = custom_palette[1:length(unique(full_data_customer$COLD_DRINK_CHANNEL))]) +  # Use custom colors
  theme_minimal() +  
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 10),  
    axis.title.x = element_text(size = 10),  
    legend.position = "none",  # Remove the legend
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(color = "gray", size = 0.5)  # Add gray vertical lines
  )

```


Dining and bulk trade are the most important channels, with customers increasing their demand by 2.1% and 5.6%, respectively, on average.

Wellness experienced the highest variation at almost 10%, but it accounts for only 3.2% of the total volume sold. Goods had the second-highest variation, at 9%, and represents 10% of the total volume.


```{r}
# Calculate the mean DEMAND_VARIATION for each COLD_DRINK_CHANNEL
channel_means <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(MEAN_DEMAND_VARIATION = mean(DEMAND_VARIATION, na.rm = TRUE))

# Merge the mean values with the full_data_customer
full_data_customer <- full_data_customer %>%
  left_join(channel_means, by = "COLD_DRINK_CHANNEL")

# Create the CHANNEL_GROWTH_POT column based on the comparison to the channel's mean DEMAND_VARIATION
full_data_customer$CHANNEL_GROWTH_POT <- ifelse(
  is.na(full_data_customer$DEMAND_VARIATION), 0,  # Set CHANNEL_GROWTH_POT to 0 if DEMAND_VARIATION is NA
  ifelse(full_data_customer$DEMAND_VARIATION > full_data_customer$MEAN_DEMAND_VARIATION, 1, 0)  # Otherwise compare to the mean
)

# Remove the MEAN_DEMAND_VARIATION column
full_data_customer <- full_data_customer %>% dplyr::select(-MEAN_DEMAND_VARIATION)


# Calculate the percentage of customers with CHANNEL_GROWTH_POT == 1 by COLD_DRINK_CHANNEL
summary_growth_channel_customers <- full_data_customer %>%
  group_by(COLD_DRINK_CHANNEL) %>%
  summarise(
    pct_high_growth = mean(CHANNEL_GROWTH_POT == 1, na.rm = TRUE) * 100  # Calculate percentage of high growth customers
  )

# Create the horizontal bar chart for the percentage of customers with CHANNEL_GROWTH_POT == 1 by COLD_DRINK_CHANNEL
ggplot(summary_growth_channel_customers, aes(x = pct_high_growth, y = reorder(COLD_DRINK_CHANNEL, pct_high_growth), fill = COLD_DRINK_CHANNEL)) +
  geom_bar(stat = "identity", position = "stack", alpha = 0.6) +
  geom_text(aes(label = paste0(round(pct_high_growth, 1), "%")),  # Round labels to 1 decimal place
            position = position_stack(vjust = 0.5), 
            hjust = -0.01, 
            color = "black", size = 3.2) +
  labs(title = "Percentage of Customers with High Growth Potential by Cold Drink Channel",
       x = "Percentage of Customers (%)", 
       y = NULL) +  
  scale_x_continuous(labels = scales::label_number(accuracy = 1), expand = expansion(c(0, 0.05))) +  # Correct scale for percentages
  scale_fill_manual(values = custom_palette[1:length(unique(full_data_customer$COLD_DRINK_CHANNEL))]) +  # Use custom colors
  theme_minimal() +  
  theme(
    plot.title = element_text(size = 11, face = "bold"),
    axis.text.y = element_text(size = 10),  
    axis.title.x = element_text(size = 10),  
    legend.position = "none",  # Remove the legend
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(color = "gray", size = 0.5)  # Add gray vertical lines
  )

```

The majority of segments showed more than 30% of stores with growth above the average for their group. Only the 'Events' segment presented a lower percentage, close to 23%. These customers will be classified as high-growth in their respective segments.

The number of customers with a variation higher than the average for each cold drink channel significantly expands the high-potential customer base. Even when simulating the number of customers with 100% growth above the average, the base was still elevated. Therefore, this criterion will need further analysis before potentially being considered.


## 8. Correlations

**Customer Features X RFM_SCORE**

Seeking to understand how the variables correlate, based on our understanding of the dataset and with the goal of obtaining clear information without multicollinearity, we chose to select numeric variables and display only the most significant correlations (disregarding the range between -0.2 and 0.2).

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}

# List of selected variables
selected_vars <- c(
  "LOCAL_FOUNT_ONLY", "LOCAL_MARKET_PARTNER", "CO2_CUSTOMER", "CHAIN_MEMBER", 
  "DAYS_ONBOARDING", "DAYS_FIRST_DLV", 
  "OT_CALL.CENTER", "OT_OTHER", "OT_SALES.REP", "OT_MYCOKE.LEGACY", "OT_MYCOKE360", "OT_EDI", 
   "RFM_SCORE", "HIGH_GROW_POT", "CHANNEL_GROWTH_POT", "LOW_DEMAND_CUST", "TOTAL_COST_CA_GAL")

# Select only the numeric variables from the dataset
numeric_vars <- full_data_customer %>%
  dplyr::select(all_of(selected_vars)) %>%
  dplyr::select(where(is.numeric))

# Compute the correlation matrix (handling missing values)
cor_matrix <- cor(numeric_vars, use = "pairwise.complete.obs")

# Replace NAs in correlation matrix with 0 to avoid errors
cor_matrix[is.na(cor_matrix)] <- 0

# Remove variables with a perfect correlation of 1
cor_matrix[cor_matrix == 1] <- NA  # Set correlations of 1 to NA to exclude them

# Convert correlation matrix to long format
cor_df <- as.data.frame(cor_matrix) %>%
  rownames_to_column(var = "Variable1") %>%
  pivot_longer(cols = -Variable1, names_to = "Variable2", values_to = "Correlation") %>%
  filter(!is.na(Correlation)) %>%  # Remove NAs which represent correlations of 1
  filter((Correlation >= 0.20 & Correlation <= 0.99) | (Correlation <= -0.20 & Correlation >= -0.99)) %>%  # Keep only correlations outside of the -0.20 to 0.20 range
  mutate(Correlation = round(Correlation, 2)) %>%  # Round correlations to 2 decimal places
  mutate(pair_id = paste0(pmin(Variable1, Variable2), "-", pmax(Variable1, Variable2))) %>%
  distinct(pair_id, .keep_all = TRUE) %>%  # Remove duplicate pairs (A-B, B-A)
  dplyr::select(Variable1, Variable2, Correlation) %>%
  arrange(desc(Correlation))  # Sort by correlation value from highest to lowest

# Display the correlation matrix in kable format with styling
cor_df %>%
  kable("html", col.names = c("Variable 1", "Variable 2", "Correlation")) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:3, width = "6em") %>%
  row_spec(0, bold = TRUE, color = "black", background = "lightgray") %>%
  add_header_above(c("Correlations Between Selected Variables (over -+0.2)" = 3)) %>%
  kable_paper("striped", full_width = FALSE)

```

```{r}
# Compute the correlation matrix for the selected numeric variables
correlation_matrix <- cor(numeric_vars, use = "pairwise.complete.obs")

# Replace NAs with 0 to avoid errors
correlation_matrix[is.na(correlation_matrix)] <- 0

# Visualize the correlation matrix with rotated labels
corrplot(correlation_matrix, method = "circle", type = "upper", 
         tl.cex = 0.8, tl.col = "black", tl.srt = 45, number.cex = 0.6, 
         diag = FALSE,  # Remove diagonal
         col = colorRampPalette(c("blue", "white", "red"))(200)) # Color palette
```

The strongest correlations were observed between Days Onboarding and Days After First Delivery (0.7) and between the order types MyCoke Legacy and MyCoke 360 (0.66). Both relationships make sense: customers who onboarded earlier tend to have older orders, except for cases where a new store belongs to an established chain. Similarly, customers who previously used the legacy channel transitioned to the newer 360 platform.

There is a correlation of 0.53 between overall customer growth and growth within the Cold Drink Channel, suggesting that expansion trends align across segments. The RFM Score also correlates with various variables that were not directly considered in its calculation, with correlations ranging from 0.44 to 0.27.

Among the negative correlations, the most notable is between RFM Score and Low Demand Customer (-0.65), indicating that lower RFM scores effectively capture low-demand customers.

**Census X Total Ordered**

All the correlations between the 2023 updated census data showed very low correlations, close to zero, in relation to the customers' consumption patterns.

For this reason, we will not work with these variables and will also remove those no longer needed to streamline full_data_customer. However, we will keep the process in this document, as the company may obtain different results when applying real locations.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
full_data_customer <- full_data_customer %>%
  dplyr::select(-c("TRANSACTIONS_DATE_COUNT", "DLVT_BOTH", "DLVT_CASES", "DLVT_GALLONS", 
            "DLVT_ORDER_LOAD", "DLVT_RETURN_GALLONS", "DLVT_RETURN_CASES", 
            "DLVT_RETURN_BOTH", "TOTAL_CASES_ORDERED", "TOTAL_CASES_DELIVERED", 
            "TOTAL_GALLONS_ORDERED", "TOTAL_GALLONS_DELIVERED", 
            "TOTAL_CASES_RETURNED", "TOTAL_GALLONS_RETURNED", "TRANS_DLV_CA_2023_01", 
            "TRANS_DLV_CA_2023_02", "TRANS_DLV_CA_2023_03", "TRANS_DLV_CA_2023_04", 
            "TRANS_DLV_CA_2023_05", "TRANS_DLV_CA_2023_06", "TRANS_DLV_CA_2023_07", 
            "TRANS_DLV_CA_2023_08", "TRANS_DLV_CA_2023_09", "TRANS_DLV_CA_2023_10", 
            "TRANS_DLV_CA_2023_11", "TRANS_DLV_CA_2023_12", "TRANS_DLV_CA_2024_01", 
            "TRANS_DLV_CA_2024_02", "TRANS_DLV_CA_2024_03", "TRANS_DLV_CA_2024_04", 
            "TRANS_DLV_CA_2024_05", "TRANS_DLV_CA_2024_06", "TRANS_DLV_CA_2024_07", 
            "TRANS_DLV_CA_2024_08", "TRANS_DLV_CA_2024_09", "TRANS_DLV_CA_2024_10", 
            "TRANS_DLV_CA_2024_11", "TRANS_DLV_CA_2024_12", "TRANS_DLV_GAL_2023_01", 
            "TRANS_DLV_GAL_2023_02", "TRANS_DLV_GAL_2023_03", "TRANS_DLV_GAL_2023_04", 
            "TRANS_DLV_GAL_2023_05", "TRANS_DLV_GAL_2023_06", "TRANS_DLV_GAL_2023_07", 
            "TRANS_DLV_GAL_2023_08", "TRANS_DLV_GAL_2023_09", "TRANS_DLV_GAL_2023_10", 
            "TRANS_DLV_GAL_2023_11", "TRANS_DLV_GAL_2023_12", "TRANS_DLV_GAL_2024_01", 
            "TRANS_DLV_GAL_2024_02", "TRANS_DLV_GAL_2024_03", "TRANS_DLV_GAL_2024_04", 
            "TRANS_DLV_GAL_2024_05", "TRANS_DLV_GAL_2024_06", "TRANS_DLV_GAL_2024_07", 
            "TRANS_DLV_GAL_2024_08", "TRANS_DLV_GAL_2024_09", "TRANS_DLV_GAL_2024_10", 
            "TRANS_DLV_GAL_2024_11", "TRANS_DLV_GAL_2024_12", "TRANS_ORD_CA_2023_01", 
            "TRANS_ORD_CA_2023_02", "TRANS_ORD_CA_2023_03", "TRANS_ORD_CA_2023_04", 
            "TRANS_ORD_CA_2023_05", "TRANS_ORD_CA_2023_06", "TRANS_ORD_CA_2023_07", 
            "TRANS_ORD_CA_2023_08", "TRANS_ORD_CA_2023_09", "TRANS_ORD_CA_2023_10", 
            "TRANS_ORD_CA_2023_11", "TRANS_ORD_CA_2023_12", "TRANS_ORD_CA_2024_01", 
            "TRANS_ORD_CA_2024_02", "TRANS_ORD_CA_2024_03", "TRANS_ORD_CA_2024_04", 
            "TRANS_ORD_CA_2024_05", "TRANS_ORD_CA_2024_06", "TRANS_ORD_CA_2024_07", 
            "TRANS_ORD_CA_2024_08", "TRANS_ORD_CA_2024_09", "TRANS_ORD_CA_2024_10", 
            "TRANS_ORD_CA_2024_11", "TRANS_ORD_CA_2024_12", "TRANS_ORD_GAL_2023_01", 
            "TRANS_ORD_GAL_2023_02", "TRANS_ORD_GAL_2023_03", "TRANS_ORD_GAL_2023_04", 
            "TRANS_ORD_GAL_2023_05", "TRANS_ORD_GAL_2023_06", "TRANS_ORD_GAL_2023_07", 
            "TRANS_ORD_GAL_2023_08", "TRANS_ORD_GAL_2023_09", "TRANS_ORD_GAL_2023_10", 
            "TRANS_ORD_GAL_2023_11", "TRANS_ORD_GAL_2023_12", "TRANS_ORD_GAL_2024_01", 
            "TRANS_ORD_GAL_2024_02", "TRANS_ORD_GAL_2024_03", "TRANS_ORD_GAL_2024_04", 
            "TRANS_ORD_GAL_2024_05", "TRANS_ORD_GAL_2024_06", "TRANS_ORD_GAL_2024_07", 
            "TRANS_ORD_GAL_2024_08", "TRANS_ORD_GAL_2024_09", "TRANS_ORD_GAL_2024_10", 
            "TRANS_ORD_GAL_2024_11", "TRANS_ORD_GAL_2024_12", "TRANS_RET_CA_2023_01", 
            "TRANS_RET_CA_2023_02", "TRANS_RET_CA_2023_03", "TRANS_RET_CA_2023_04", 
            "TRANS_RET_CA_2023_05", "TRANS_RET_CA_2023_06", "TRANS_RET_CA_2023_07", 
            "TRANS_RET_CA_2023_08", "TRANS_RET_CA_2023_09", "TRANS_RET_CA_2023_10", 
            "TRANS_RET_CA_2023_11", "TRANS_RET_CA_2023_12", "TRANS_RET_CA_2024_01", 
            "TRANS_RET_CA_2024_02", "TRANS_RET_CA_2024_03", "TRANS_RET_CA_2024_04", 
            "TRANS_RET_CA_2024_05", "TRANS_RET_CA_2024_06", "TRANS_RET_CA_2024_07", 
            "TRANS_RET_CA_2024_08", "TRANS_RET_CA_2024_09", "TRANS_RET_CA_2024_10", 
            "TRANS_RET_CA_2024_11", "TRANS_RET_CA_2024_12", "TRANS_RET_GAL_2023_01", 
            "TRANS_RET_GAL_2023_02", "TRANS_RET_GAL_2023_03", "TRANS_RET_GAL_2023_04", 
            "TRANS_RET_GAL_2023_05", "TRANS_RET_GAL_2023_06", "TRANS_RET_GAL_2023_07", 
            "TRANS_RET_GAL_2023_08", "TRANS_RET_GAL_2023_09", "TRANS_RET_GAL_2023_10", 
            "TRANS_RET_GAL_2023_11", "TRANS_RET_GAL_2023_12", "TRANS_RET_GAL_2024_01", 
            "TRANS_RET_GAL_2024_02", "TRANS_RET_GAL_2024_03", "TRANS_RET_GAL_2024_04", 
            "TRANS_RET_GAL_2024_05", "TRANS_RET_GAL_2024_06", "TRANS_RET_GAL_2024_07", 
            "TRANS_RET_GAL_2024_08", "TRANS_RET_GAL_2024_09", "TRANS_RET_GAL_2024_10", 
            "TRANS_RET_GAL_2024_11", "TRANS_RET_GAL_2024_12", "QTD_DLV_CA_2023_01", 
            "QTD_DLV_CA_2023_02", "QTD_DLV_CA_2023_03", "QTD_DLV_CA_2023_04", 
            "QTD_DLV_CA_2023_05", "QTD_DLV_CA_2023_06", "QTD_DLV_CA_2023_07", 
            "QTD_DLV_CA_2023_08", "QTD_DLV_CA_2023_09", "QTD_DLV_CA_2023_10", 
            "QTD_DLV_CA_2023_11", "QTD_DLV_CA_2023_12", "QTD_DLV_CA_2024_01", 
            "QTD_DLV_CA_2024_02", "QTD_DLV_CA_2024_03", "QTD_DLV_CA_2024_04", 
            "QTD_DLV_CA_2024_05", "QTD_DLV_CA_2024_06", "QTD_DLV_CA_2024_07", 
            "QTD_DLV_CA_2024_08", "QTD_DLV_CA_2024_09", "QTD_DLV_CA_2024_10", 
            "QTD_DLV_CA_2024_11", "QTD_DLV_CA_2024_12", "QTD_DLV_GAL_2023_01", 
            "QTD_DLV_GAL_2023_02", "QTD_DLV_GAL_2023_03", "QTD_DLV_GAL_2023_04", 
            "QTD_DLV_GAL_2023_05", "QTD_DLV_GAL_2023_06", "QTD_DLV_GAL_2023_07", 
            "QTD_DLV_GAL_2023_08", "QTD_DLV_GAL_2023_09", "QTD_DLV_GAL_2023_10", 
            "QTD_DLV_GAL_2023_11", "QTD_DLV_GAL_2023_12", "QTD_DLV_GAL_2024_01", 
            "QTD_DLV_GAL_2024_02", "QTD_DLV_GAL_2024_03", "QTD_DLV_GAL_2024_04", 
            "QTD_DLV_GAL_2024_05", "QTD_DLV_GAL_2024_06", "QTD_DLV_GAL_2024_07", 
            "QTD_DLV_GAL_2024_08", "QTD_DLV_GAL_2024_09", "QTD_DLV_GAL_2024_10", 
            "QTD_DLV_GAL_2024_11", "QTD_DLV_GAL_2024_12", "QTD_ORD_CA_2023_01", 
            "QTD_ORD_CA_2023_02", "QTD_ORD_CA_2023_03", "QTD_ORD_CA_2023_04", 
            "QTD_ORD_CA_2023_05", "QTD_ORD_CA_2023_06", "QTD_ORD_CA_2023_07", 
            "QTD_ORD_CA_2023_08", "QTD_ORD_CA_2023_09", "QTD_ORD_CA_2023_10", 
            "QTD_ORD_CA_2023_11", "QTD_ORD_CA_2023_12", "QTD_ORD_CA_2024_01", 
            "QTD_ORD_CA_2024_02", "QTD_ORD_CA_2024_03", "QTD_ORD_CA_2024_04", 
            "QTD_ORD_CA_2024_05", "QTD_ORD_CA_2024_06", "QTD_ORD_CA_2024_07", 
            "QTD_ORD_CA_2024_08", "QTD_ORD_CA_2024_09", "QTD_ORD_CA_2024_10", 
            "QTD_ORD_CA_2024_11", "QTD_ORD_CA_2024_12", "QTD_ORD_GAL_2023_01", 
            "QTD_ORD_GAL_2023_02", "QTD_ORD_GAL_2023_03", "QTD_ORD_GAL_2023_04", 
            "QTD_ORD_GAL_2023_05", "QTD_ORD_GAL_2023_06", "QTD_ORD_GAL_2023_07", 
            "QTD_ORD_GAL_2023_08", "QTD_ORD_GAL_2023_09", "QTD_ORD_GAL_2023_10", 
            "QTD_ORD_GAL_2023_11", "QTD_ORD_GAL_2023_12", "QTD_ORD_GAL_2024_01", 
            "QTD_ORD_GAL_2024_02", "QTD_ORD_GAL_2024_03", "QTD_ORD_GAL_2024_04", 
            "QTD_ORD_GAL_2024_05", "QTD_ORD_GAL_2024_06", "QTD_ORD_GAL_2024_07", 
            "QTD_ORD_GAL_2024_08", "QTD_ORD_GAL_2024_09", "QTD_ORD_GAL_2024_10", 
            "QTD_ORD_GAL_2024_11", "QTD_ORD_GAL_2024_12", "QTD_RET_CA_2023_01", 
            "QTD_RET_CA_2023_02", "QTD_RET_CA_2023_03", "QTD_RET_CA_2023_04", 
            "QTD_RET_CA_2023_05", "QTD_RET_CA_2023_06", "QTD_RET_CA_2023_07", 
            "QTD_RET_CA_2023_08", "QTD_RET_CA_2023_09", "QTD_RET_CA_2023_10", 
            "QTD_RET_CA_2023_11", "QTD_RET_CA_2023_12", "QTD_RET_CA_2024_01", 
            "QTD_RET_CA_2024_02", "QTD_RET_CA_2024_03", "QTD_RET_CA_2024_04", 
            "QTD_RET_CA_2024_05", "QTD_RET_CA_2024_06", "QTD_RET_CA_2024_07", 
            "QTD_RET_CA_2024_08", "QTD_RET_CA_2024_09", "QTD_RET_CA_2024_10", 
            "QTD_RET_CA_2024_11", "QTD_RET_CA_2024_12", "QTD_RET_GAL_2023_01", 
            "QTD_RET_GAL_2023_02", "QTD_RET_GAL_2023_03", "QTD_RET_GAL_2023_04", 
            "QTD_RET_GAL_2023_05", "QTD_RET_GAL_2023_06", "QTD_RET_GAL_2023_07", 
            "QTD_RET_GAL_2023_08", "QTD_RET_GAL_2023_09", "QTD_RET_GAL_2023_10", 
            "QTD_RET_GAL_2023_11", "QTD_RET_GAL_2023_12", "QTD_RET_GAL_2024_01", 
            "QTD_RET_GAL_2024_02", "QTD_RET_GAL_2024_03", "QTD_RET_GAL_2024_04", 
            "QTD_RET_GAL_2024_05", "QTD_RET_GAL_2024_06", "QTD_RET_GAL_2024_07", 
            "QTD_RET_GAL_2024_08", "QTD_RET_GAL_2024_09", "QTD_RET_GAL_2024_10", 
            "QTD_RET_GAL_2024_11", "QTD_RET_GAL_2024_12")) 

# List all variables in the environment
all_vars <- ls()

# Exclude 'full_data', 'full_data_customer', and the new variables from removal
vars_to_keep <- c("full_data", "full_data_customer","mydir", "one_seed", "reference_date")

# Get the variables to remove
vars_to_remove <- setdiff(all_vars, vars_to_keep)

# Remove the temporary data frames
rm(list = vars_to_remove)

# Clean up by removing 'all_vars' and 'vars_to_remove'
rm(all_vars, vars_to_remove)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Save full_data as CSV
#write.csv(full_data, file = "full_data.csv", row.names = FALSE)

# Save full_data_customer as CSV
#write.csv(full_data_customer, file = "full_data_customer.csv", row.names = FALSE)

# Load the full_data CSV
#full_data <- read.csv(file = "full_data.csv", sep = ",", stringsAsFactors = FALSE)

# Load the full_data_customer CSV
#full_data_customer <- read.csv(file = "full_data_customer.csv", sep = ",", stringsAsFactors = FALSE)

```


## 9. Customer Segmentation

Since all customers in the original data are served by red trucks, there is no prior information on the characteristics of those who will be served by white trucks. The only reference is the average annual consumption threshold of 400 gallons or cases.  

Therefore, we will segment these customers to group them based on their most relevant characteristics within the available scope, including the ones we created.  

We chose to select variables that represent the characteristics or consumption habits of each store while excluding geographic information and census data.

Below are the variables we selected:  

**Customer Type & Relationship**:  
These variables represent customers' relationship with the company and their type:  
- LOCAL_FOUNT_ONLY: Customers who only consume fountain drinks.  
- LOCAL_MARKET_PARTNER: Local market partners.  
- CO2_CUSTOMER: Customers who are CO2 consumers.  
- CHAIN_MEMBER: Customers who are part of a chain.  

**Time-Related Metrics**:  
Time-related metrics track customers' activity and engagement over time:  
- DAYS_ONBOARDING: Number of days since onboarding.  
- DAYS_FIRST_DLV: Number of days since the first delivery.  
- DAYS_AF_LAST_ORD: Number of days after the last order.  
- AVG_DAYS_BET_ORD: Average number of days between orders.  

**Order & Sales Behavior**:  
These variables represent customer behaviors in terms of orders and sales:  
- NUM_ORDERS: Total number of orders.  
- TOTAL_ORDERED: Total volume of orders.  
- RFM_SCORE: Adapted Recency, Frequency, Monetary score.  
- TOTAL_COST_CA_GAL: Total cost in deliveries for 2023 and 2024.  

**Order Channels**:  
This category contains data on the various channels through which customers make their transactions:  
- OT_CALL.CENTER: Transactions via call center.  
- OT_OTHER: Transactions made through other means (emails, trade fairs, etc.).  
- OT_SALES.REP: Transactions via sales representatives.  
- OT_MYCOKE: Transactions via MyCoke (legacy platform).  
- OT_EDI: Transactions via electronic direct ordering (EDI).  

**Growth & Demand Potential**:  
These flags indicate customers' growth and demand potential:  
- HIGH_GROW_POT: Flag for customers with above-average growth potential across all segments.  
- CHANNEL_GROWTH_POT: Flag for customers with above-average growth within their segment.  
- LOW_DEMAND_CUST: Flag for customers with low demand (below the 1st quartile) by segment.

Three variables have a wide range of values with extreme outliers. For these variables—**NUM_ORDERS**, **TOTAL_ORDERED**, and **TOTAL_COST_CA_GAL**—we will apply a logarithmic transformation.

```{r, warning=FALSE}
# Select the primary variables for clustering based on business relevance
selected_vars <- c(
  "LOCAL_FOUNT_ONLY", "LOCAL_MARKET_PARTNER", "CO2_CUSTOMER", "CHAIN_MEMBER", 
  "DAYS_ONBOARDING", "DAYS_FIRST_DLV", "DAYS_AF_LAST_ORD", "AVG_DAYS_BET_ORD", 
  "OT_CALL.CENTER", "OT_OTHER", "OT_SALES.REP", "OT_MYCOKE.LEGACY", "OT_MYCOKE360", "OT_EDI", 
  "NUM_ORDERS", "TOTAL_ORDERED", "RFM_SCORE", "HIGH_GROW_POT", "CHANNEL_GROWTH_POT", "LOW_DEMAND_CUST", "TOTAL_COST_CA_GAL")

# Extract the data and apply log transformation to NUM_ORDERS and TOTAL_ORDERED
data_to_cluster <- full_data_customer %>%
  dplyr::select(all_of(selected_vars)) %>%
  dplyr::select(where(is.numeric))

# Apply log transformation directly on the selected numeric variables
data_to_cluster$DAYS_ONBOARDING <- log1p(data_to_cluster$DAYS_ONBOARDING)
data_to_cluster$DAYS_FIRST_DLV <- log1p(data_to_cluster$DAYS_FIRST_DLV)
data_to_cluster$TOTAL_ORDERED <- log1p(data_to_cluster$TOTAL_ORDERED)
data_to_cluster$TOTAL_COST_CA_GAL <- log1p(data_to_cluster$TOTAL_COST_CA_GAL)

# Standardize the numeric variables for clustering
data_to_cluster <- scale(data_to_cluster)

# Determine optimal number of clusters using the Elbow Method
set.seed(500)  
wss <- sapply(1:10, function(k) kmeans(data_to_cluster, centers = k, nstart = 25)$tot.withinss)

# Visualize the Elbow Method results
plot(1:10, wss, type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of Clusters", ylab = "Total Within Sum of Squares (WSS)", 
     main = "Elbow Method for Optimal K")


```

After testing different compositions to calculate the silhouette score and ARI score, including cluster numbers from 2 to 4, various distance metrics (Euclidean, Manhattan), and different algorithms (Hartigan-Wong, MacQueen, Lloyd), we present the most relevant metrics.

```{r, warning=FALSE}
# Set seed for reproducibility
set.seed(500)

# Function to calculate the Silhouette Score
calculate_silhouette_score <- function(model, data) {
  clusters <- model$cluster  
  if (length(clusters) != nrow(data)) {
    stop("Cluster assignments do not match the number of data points.")
  }
  dist_matrix <- dist(data)
  silhouette_score <- silhouette(clusters, dist_matrix)
  return(mean(silhouette_score[, 3]))  
}

# Function to calculate Adjusted Rand Index
calculate_ari <- function(model, true_labels) {
  clusters <- model$cluster
  ari_score <- adjustedRandIndex(clusters, true_labels)
  return(ari_score)
}

# Select the primary variables for clustering based on business relevance
selected_vars <- c(
  "LOCAL_FOUNT_ONLY", "LOCAL_MARKET_PARTNER", "CO2_CUSTOMER", "CHAIN_MEMBER", 
  "DAYS_ONBOARDING", "DAYS_FIRST_DLV", "DAYS_AF_LAST_ORD", "AVG_DAYS_BET_ORD", 
  "OT_CALL.CENTER", "OT_OTHER", "OT_SALES.REP", "OT_MYCOKE.LEGACY", "OT_MYCOKE360", "OT_EDI", 
  "NUM_ORDERS", "TOTAL_ORDERED", "RFM_SCORE", "HIGH_GROW_POT", "CHANNEL_GROWTH_POT", "LOW_DEMAND_CUST", "TOTAL_COST_CA_GAL")

# Extract the data and apply log transformation to NUM_ORDERS and TOTAL_ORDERED
data_to_cluster <- full_data_customer %>%
  dplyr::select(all_of(selected_vars)) %>%
  dplyr::select(where(is.numeric))

# Apply log transformation directly on the selected numeric variables
data_to_cluster$DAYS_ONBOARDING <- log1p(data_to_cluster$DAYS_ONBOARDING)
data_to_cluster$DAYS_FIRST_DLV <- log1p(data_to_cluster$DAYS_FIRST_DLV)
data_to_cluster$TOTAL_ORDERED <- log1p(data_to_cluster$TOTAL_ORDERED)
data_to_cluster$TOTAL_COST_CA_GAL <- log1p(data_to_cluster$TOTAL_COST_CA_GAL)

# Standardize the numeric variables for clustering
data_to_cluster <- scale(data_to_cluster)

# Define different parameter configurations for K-means
params <- list(
  list(name = "Euclidean, 2 Clusters", centers = 2, nstart = 25, algorithm = "Hartigan-Wong"),
  list(name = "Euclidean, 3 Clusters", centers = 3, nstart = 25, algorithm = "Hartigan-Wong"),
  list(name = "Euclidean, 4 Clusters", centers = 4, nstart = 25, algorithm = "Hartigan-Wong")
#  ,
  
#  list(name = "Manhattan, 3 Clusters", centers = 2, nstart = 25, algorithm = "MacQueen"),
#  list(name = "Manhattan, 2 Clusters", centers = 3, nstart = 25, algorithm = "MacQueen"),
#  list(name = "Manhattan, 4 Clusters", centers = 4, nstart = 25, algorithm = "MacQueen"),
  
#  list(name = "K-means++, 3 Clusters", centers = 2, nstart = 25, algorithm = "Lloyd"),
#  list(name = "K-means++, 2 Clusters", centers = 3, nstart = 25, algorithm = "Lloyd"),
#  list(name = "K-means++, 4 Clusters", centers = 4, nstart = 25, algorithm = "Lloyd")
)

# Apply K-means clustering and store results
results <- lapply(params, function(param) {
  model <- kmeans(data_to_cluster, centers = param$centers, nstart = param$nstart, algorithm = param$algorithm)
  silhouette <- calculate_silhouette_score(model, data_to_cluster)
  ari <- calculate_ari(model, full_data_customer$LOCAL_MARKET_PARTNER)  # You can replace with a true label column if needed
  return(data.frame(Model = param$name, Silhouette_Score = round(silhouette, 3), ARI = round(ari, 3)))
})

# Combine results into a single table
results_df <- do.call(rbind, results)

# Display table using kable
kable(results_df, col.names = c("Parameter", "Silhouette Score", "Adjusted Rand Index (ARI)"))

```

Given the results, we chose to use "Euclidean, 3 Clusters", with centers = 3, nstart = 25, and the "Hartigan-Wong" algorithm (standard), as it showed the best performance compared to the others. However, we can still consider that the separation between the clusters is marginal and weak.

Below is the visualization of the clusters based on the two principal components.

```{r}
# Implement K-means with optimal number of clusters 
set.seed(500)
kmeans_optimal <- kmeans(data_to_cluster, centers = 3, nstart = 25, algorithm = "Hartigan-Wong")

# Add cluster assignments to the original dataset
full_data_customer$CLUSTER <- kmeans_optimal$cluster

# Define custom colors for the clusters
palette_clusters <- c(
  "1" = "#FF6347",  # Coral
  "2" = "#4682B4",  # Cornflower blue
  "3" = "#FFD700")   # Yellow

# Visualize cluster distribution with PCA-reduced dimensions
fviz_cluster(kmeans_optimal, data = data_to_cluster, geom = "point", 
             ellipse.type = "none", 
             main = "Customer Segmentation: PCA-based Visualization",
             subtitle = "K-means Optimal Clustering with 3 Segments",
             ggtheme = theme_minimal()) +
  scale_color_manual(values = palette_clusters) # Manually set colors
```



The customer segmentation will be discussed later, including the interpretation of each cluster.



###  9.1 Clusters and principal components 

Given the visualization of the clusters through their principal components, we chose to better understand the characteristics of the two main components, which account for 39% of the variability.

```{r}
# Select the desired variables for clustering - adjusted to match your clustering selection
selected_vars <- c(
  "LOCAL_FOUNT_ONLY", "LOCAL_MARKET_PARTNER", "CO2_CUSTOMER", "CHAIN_MEMBER", 
  "DAYS_ONBOARDING", "DAYS_FIRST_DLV", "DAYS_AF_LAST_ORD", "AVG_DAYS_BET_ORD", 
  "OT_CALL.CENTER", "OT_OTHER", "OT_SALES.REP", "OT_MYCOKE.LEGACY", "OT_MYCOKE360", "OT_EDI", 
  "NUM_ORDERS", "TOTAL_ORDERED", "RFM_SCORE", "HIGH_GROW_POT", "CHANNEL_GROWTH_POT", "LOW_DEMAND_CUST", "TOTAL_COST_CA_GAL")

# Select only the desired variables from full_data_customer - works for both data.frame and data.table
customer_data <- full_data_customer[, selected_vars]

# Remove rows with NA values (if any)
customer_data <- na.omit(customer_data)

# Apply log transformation to the same variables as in clustering
customer_data$DAYS_ONBOARDING <- log1p(customer_data$DAYS_ONBOARDING)
customer_data$DAYS_FIRST_DLV <- log1p(customer_data$DAYS_FIRST_DLV)
customer_data$TOTAL_ORDERED <- log1p(customer_data$TOTAL_ORDERED)
customer_data$TOTAL_COST_CA_GAL <- log1p(customer_data$TOTAL_COST_CA_GAL)

# Build scales for the dataset
scales <- build_scales(customer_data, verbose = FALSE)

# Scaling columns
customer_data <- fast_scale(customer_data, scales = scales, verbose = FALSE)

# Calculating the covariance matrix
cov_customer <- cov(customer_data)

# Calculating the Eigenvector and Eigenvalues of the variance-covariance matrix
e_customer <- eigen(cov_customer)
eigenvalues_customer <- e_customer$values
eigenvectors_customer <- e_customer$vectors

# Print
#print(paste("Counts the number of eigenvalues:", length(eigenvalues_customer)))

# Initialize an empty matrix to store the contributions of variables to all PCs
contributions_matrix <- matrix(NA, nrow = ncol(customer_data), ncol = ncol(eigenvectors_customer))

# Loop through all principal components
for (i in 1:ncol(eigenvectors_customer)) {
  # Get the contributions of variables to the i-th principal component (PC)
  pc_contributions <- eigenvectors_customer[, i]
  
  # Assign the contributions to the corresponding column in the matrix
  contributions_matrix[, i] <- round(pc_contributions, 2)  # Round to 2 decimal places
}

# Convert the matrix to a data frame and assign appropriate row and column names
contributions_df <- as.data.frame(contributions_matrix)
colnames(contributions_df) <- paste0("PC", 1:ncol(contributions_matrix))  # Name the columns dynamically
rownames(contributions_df) <- colnames(customer_data)  # Assign the variable names as row names

# Variance Explained
# Calculate the variance explained by each principal component
variance_explained <- eigenvalues_customer / sum(eigenvalues_customer)

# Round the variance explained to 2 decimal places
variance_row <- round(variance_explained, 2)

# Calculate the cumulative variance explained
cumulative_variance <- cumsum(variance_explained)

# Round the cumulative variance to 2 decimal places
cumulative_variance_row <- round(cumulative_variance, 2)

# Add the variance and cumulative variance rows to the bottom of the data frame
contributions_df <- rbind(contributions_df, 
                        Variance_Explained = variance_row,
                        Cumulative_Variance = cumulative_variance_row)

# Format the table using formattable for heatmap effect
formattable(contributions_df, 
          list(
            # Apply color gradient to all columns
            area(col = 1:ncol(contributions_df)) ~ color_tile("white", "deepskyblue3")
          ))
```


Principal Component 1 has the highest weight from the variables RFM_SCORE, NUM_ORDERS, TOTAL_ORDERED, and TOTAL_COST_CA_GAL, representing 30% of the variance.  
Principal Component 2 adds another 9% of variance, with the highest weight from the OT_MYCOKE variables.

### 9.2 Clusters Features

We will characterize the clusters based on their relationships with other variables.


```{r, warning=FALSE, message=FALSE}
# Define specific colors for fleet types
fleet_colors <- c("RED TRUCK" = "#B33951", "WHITE TRUCK" = "#D3D3D3")  # Custom colors for FleetType

# Create a cross-tabulation of CLUSTER and FLEET_TYPE
cluster_fleet_table <- table(full_data_customer$CLUSTER, full_data_customer$FLEET_TYPE, 
                            useNA = "ifany")

# Create data frame for visualization
cluster_fleet_df <- as.data.frame.table(cluster_fleet_table)
names(cluster_fleet_df) <- c("Segment", "FleetType", "Count")

# Filter out NA values for cleaner visualization
cluster_fleet_df <- cluster_fleet_df %>% 
  filter(!is.na(Segment) & !is.na(FleetType))

# Calculate proportions
cluster_fleet_df$Pct <- cluster_fleet_df$Count / ave(cluster_fleet_df$Count, cluster_fleet_df$Segment, FUN = sum)

# Create percentage distribution plot for fleet types within clusters
ggplot(cluster_fleet_df, aes(x = Segment, y = Pct, fill = FleetType)) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = fleet_colors) +  # Use custom colors for fleet types
  geom_text(aes(label = scales::percent(Pct, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5), 
            color = "black", size = 3.5) +  # Add percentage text labels inside the bars
  labs(title = "Fleet Type Distribution Across Clusters",
       subtitle = "Fleet type classification using a 400-gallon threshold",
       x = "Cluster",
       y = "Percentage",
       fill = "Fleet Type") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    legend.position = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )

# Define specific colors for the other variables
high_growth_colors <- c("1" = "#FF6347", "0" = "#D3D3D3")  # High Growth vs Low Growth
fountain_only_colors <- c("1" = "#4682B4", "0" = "#D3D3D3")  # Fountain Only vs Not Fountain Only

# Create data frame for HIGH_GROW_POT visualization
cluster_high_growth_df <- as.data.frame.table(table(full_data_customer$CLUSTER, full_data_customer$HIGH_GROW_POT))
names(cluster_high_growth_df) <- c("Segment", "HighGrowth", "Count")

# Filter out NA values for cleaner visualization
cluster_high_growth_df <- cluster_high_growth_df %>% 
  filter(!is.na(Segment) & !is.na(HighGrowth))

# Calculate proportions for HIGH_GROW_POT
cluster_high_growth_df$Pct <- cluster_high_growth_df$Count / ave(cluster_high_growth_df$Count, cluster_high_growth_df$Segment, FUN = sum)

# Plot for HIGH_GROW_POT distribution by clusters
ggplot(cluster_high_growth_df, aes(x = Segment, y = Pct, fill = HighGrowth)) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = high_growth_colors) +  # Custom colors for HIGH_GROW_POT
  geom_text(aes(label = scales::percent(Pct, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5), 
            color = "black", size = 3.5) +  # Add percentage text labels inside the bars
  labs(title = "Clusters by Growth Potential",
       subtitle = "Proportional Representation by High Growth Potential",
       x = "Cluster",
       y = "Percentage",
       fill = "Growth Potential") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    legend.position = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )

# Create data frame for LOCAL_FOUNT_ONLY visualization
cluster_fountain_df <- as.data.frame.table(table(full_data_customer$CLUSTER, full_data_customer$LOCAL_FOUNT_ONLY))
names(cluster_fountain_df) <- c("Segment", "FountainOnly", "Count")

# Filter out NA values for cleaner visualization
cluster_fountain_df <- cluster_fountain_df %>% 
  filter(!is.na(Segment) & !is.na(FountainOnly))

# Calculate proportions for LOCAL_FOUNT_ONLY
cluster_fountain_df$Pct <- cluster_fountain_df$Count / ave(cluster_fountain_df$Count, cluster_fountain_df$Segment, FUN = sum)

# Plot for LOCAL_FOUNT_ONLY distribution by clusters
ggplot(cluster_fountain_df, aes(x = Segment, y = Pct, fill = FountainOnly)) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = fountain_only_colors) +  # Custom colors for LOCAL_FOUNT_ONLY
  geom_text(aes(label = scales::percent(Pct, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5), 
            color = "black", size = 3.5) +  # Add percentage text labels inside the bars
  labs(title = "Clusters by Fountain Only",
       subtitle = "Proportional Representation by Fountain Only",
       x = "Cluster",
       y = "Percentage",
       fill = "Fountain Only") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    legend.position = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )

# Define specific colors for the LOW_DEMAND_CUST variable
low_demand_colors <- c("1" = "yellow", "0" = "#D3D3D3")  # Low Demand vs Not Low Demand

# Create data frame for LOW_DEMAND_CUST visualization
cluster_low_demand_df <- as.data.frame.table(table(full_data_customer$CLUSTER, full_data_customer$LOW_DEMAND_CUST))
names(cluster_low_demand_df) <- c("Segment", "LowDemand", "Count")

# Filter out NA values for cleaner visualization
cluster_low_demand_df <- cluster_low_demand_df %>% 
  filter(!is.na(Segment) & !is.na(LowDemand))

# Calculate proportions for LOW_DEMAND_CUST
cluster_low_demand_df$Pct <- cluster_low_demand_df$Count / ave(cluster_low_demand_df$Count, cluster_low_demand_df$Segment, FUN = sum)

# Plot for LOW_DEMAND_CUST distribution by clusters
ggplot(cluster_low_demand_df, aes(x = Segment, y = Pct, fill = LowDemand)) +
  geom_bar(stat = "identity", position = "fill", width = 0.7) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = low_demand_colors) +  # Custom colors for LOW_DEMAND_CUST
  geom_text(aes(label = scales::percent(Pct, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5), 
            color = "black", size = 3.5) +  # Add percentage text labels inside the bars
  labs(title = "Clusters by Low Demand Customers",
       subtitle = "Proportional Representation by Low Demand Customers",
       x = "Cluster",
       y = "Percentage",
       fill = "Low Demand Customers") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    legend.position = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )



# Define colors from the custom palette for clusters
# Reshape data for faceting
plot_data <- melt(full_data_customer, id.vars = "CLUSTER", measure.vars = c("RFM_SCORE", "NUM_ORDERS", "TOTAL_ORDERED"))

# Create a new variable for log-transformed TOTAL_ORDERED
full_data_customer$LOG_TOTAL_ORDERED <- log1p(full_data_customer$TOTAL_ORDERED)  # log1p to handle zero values

# Reshape data using tidyr::pivot_longer()
plot_data <- full_data_customer %>%
  pivot_longer(cols = c(RFM_SCORE, NUM_ORDERS, LOG_TOTAL_ORDERED),
               names_to = "variable", values_to = "value")

# Rename variable levels for better readability
plot_data$variable <- case_when(
  plot_data$variable == "LOG_TOTAL_ORDERED" ~ "TOTAL_ORDERED (Log Scale)",
  TRUE ~ plot_data$variable
)

# Create a boxplot with facet_wrap
ggplot(plot_data, aes(x = factor(CLUSTER), y = value, fill = factor(CLUSTER))) +
  geom_boxplot(color = "black", alpha = 0.7) +  # Add black borders for contrast
  scale_fill_manual(values = palette_clusters) +  # Apply custom colors for clusters
  facet_wrap(~ variable, scales = "free_y") +  # Allow different y-scales per variable
  labs(
    title = "Customer Segmentation Characterization", 
    subtitle = "Distribution of RFM Score, Number of Orders, and Log-Transformed Total Ordered for each cluster",
    x = "Cluster", 
    y = "Value"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray30"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_blank(),
    legend.position = "none"  # Remove legend since clusters are already labeled on the x-axis
  )


```
```{r, warning=FALSE}
# Prepare the dataset
plot_data_filtered <- full_data_customer %>%
  pivot_longer(cols = c("OT_CALL.CENTER", "OT_OTHER", "OT_SALES.REP", 
                        "OT_MYCOKE.LEGACY", "OT_MYCOKE360", "OT_EDI"),
               names_to = "variable", values_to = "value") %>%
  mutate(value = log1p(value))  # Log-transform values safely to handle zero values

# Define custom labels for the variables
custom_labels <- c(
  "OT_CALL.CENTER" = "Call Center",
  "OT_OTHER" = "Other",
  "OT_SALES.REP" = "Sales Rep",
  "OT_MYCOKE.LEGACY" = "MyCoke Legacy",
  "OT_MYCOKE360" = "MyCoke360",
  "OT_EDI" = "EDI"
)

# Generate the boxplot
ggplot(plot_data_filtered, aes(x = factor(CLUSTER), y = value, fill = factor(CLUSTER))) +
  geom_boxplot(color = "black", alpha = 0.7) +  # Add black borders for contrast
  scale_fill_manual(values = palette_clusters) +  # Apply custom colors for clusters
  facet_wrap(~ variable, scales = "fixed", labeller = labeller(variable = as_labeller(custom_labels))) + 
  scale_y_continuous(limits = c(0, 6), breaks = seq(0, 6, 1)) +  # Set fixed scale for y-axis
  labs(
  title = "Customer Segmentation Characterization", 
  subtitle = "Distribution of orders by Order Type (Log Scale) for each cluster",
    x = "Cluster",
    y = "Log(Value)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray30"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_blank(),
    legend.position = "none"  # Remove legend since clusters are already labeled on the x-axis
  )

```

**Cluster 1: Less Active Customers with Low Order Volume**  
- Composition: Only 0.4% of customers receive deliveries via red trucks (based on the benchmark threshold of 400 gallons on average per year).  
- Growth: Approximately 6% of customers exhibit high growth potential.  
- Local Fountain Only: This cluster has the highest percentage of local fountain-only customers, at 7.5%.  
- Average RFM: The average RFM score is 7, indicating these are the least active customers.  
- Average Number of Orders: The average number of orders per customer was 5.5 in 2023 and 2024.  
- Total Ordered Volume: The average total ordered volume per customer in 2023 and 2024 is around 80 gallons, while the median is 57 gallons, indicating a large number of customers with smaller volumes.  
- Volume Share: This cluster represents only 1.7% of the total volume consumed in 2023 and 2024.  
- The cluster shows orders concentrated through call centers, digital channels, and sales representatives, although in smaller absolute quantities compared to the other clusters.

**Cluster 2: High Demand Customers**  
- Composition: Approximately 80% of customers receive deliveries via red trucks (based on the benchmark threshold of 400 gallons on average per year).  
- Growth: Around 7% of customers exhibit high growth potential.  
- Local Fountain Only: Only 1.5% of customers are local fountain-only.  
- Average RFM: The average RFM score for this cluster is 29, the highest among the three clusters.  
- Average Number of Orders: The average number of orders per customer was 81 in 2023 and 2024, with many outliers showing significantly higher order volumes.  
- Total Ordered Volume: The average total ordered volume per customer in 2023 and 2024 is 4,638 gallons. This cluster has the highest number of outliers with elevated volumes, which skews the average. The volume representing the median is 1,707 gallons. 
- Volume Share: This cluster represents 76% of the total volume consumed in 2023 and 2024.  
- It has the highest number of orders through digital channels and is the cluster most served by sales representatives.

**Cluster 3: Intermediate Customers with Growth Potential**  
- Composition: Approximately 87% of customers receive deliveries via white trucks (based on the benchmark threshold of 400 gallons on average per year).  
- Growth: This cluster has the highest percentage of customers with high growth potential, at 16.6%.  
- Local Fountain Only: Around 4.2% of customers are local fountain-only.  
- Average RFM: The average RFM score for this group is 18.7, the second highest among the clusters.  
- Average Number of Orders: The average number of orders per customer was 30 in 2023 and 2024.  
- Total Ordered Volume: The average total ordered volume per customer in 2023 and 2024 is 525 gallons. The median volume is 331 gallons.  
- Volume Share: This cluster represents approximately 22% of the total volume consumed in 2023 and 2024.  
- It is the cluster with the highest average number of orders placed via the call center. It has fewer orders through digital channels compared to Cluster 2, but more than Cluster 1. The number of orders through sales representatives is similar to Cluster 1


## 10. Classification Models for Explaining Clusters

To better understand the variables influencing cluster composition and facilitate future predictions without the need for re-clustering, we will use two classification models: decision trees and multinomial logistic regression. These models will help identify the key characteristics that drive cluster formation.

By applying these models to new data, we can predict cluster assignments, streamlining the analysis process and eliminating the need to recreate the clusters whenever new data is introduced.

### 10.1 Decision Tree

We will analyze how the selected variables explain the clusters. To do this, we will use a decision tree, splitting the dataset into training and test sets with 20-fold cross-validation.


```{r}
# Prepare data for decision tree
model_data <- full_data_customer %>%
  dplyr::select(all_of(selected_vars), CLUSTER) %>%
  mutate(CLUSTER = as.factor(CLUSTER)) 

# Create train/test split (70% train, 30% test)
set.seed(500)  
train_indices <- createDataPartition(model_data$CLUSTER, p = 0.7, list = FALSE)
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Set up cross-validation (20-fold)
train_control <- trainControl(method = "cv", number = 20)

# Train the decision tree model with cross-validation
decision_tree_model <- train(
  CLUSTER ~ ., 
  data = train_data,  
  method = "rpart", 
  trControl = train_control,
  tuneLength = 5)  

# Plot the decision tree
rpart.plot(
  decision_tree_model$finalModel, 
  extra = 101,                
  box.palette = "Blues",      
  shadow.col = "gray",        
  nn = TRUE,                  
  main = "Decision Tree: Explaining Customer Clusters", 
  branch.col = "gray",        
  faclen = 0,
  tweak = 1.1)                  

```
Below are the prediction performance metrics:


```{r}
# Evaluate model performance on test set
dt_test_predictions <- predict(decision_tree_model, test_data, type = "raw")
dt_test_confusion_matrix <- confusionMatrix(dt_test_predictions, test_data$CLUSTER)

# Calculate accuracy on the test set
dt_test_accuracy <- round(mean(dt_test_predictions == test_data$CLUSTER), 2)

# Evaluate model performance on train set
dt_train_predictions <- predict(decision_tree_model, train_data, type = "raw")
dt_train_accuracy <- round(mean(dt_train_predictions == train_data$CLUSTER), 2)

# Print model performance metrics
cat("\n--- Decision Tree Model Performance ---\n")
print(dt_test_confusion_matrix)

```
```{r}
# Evaluate accuracy on train and test sets for decision tree
dt_train_acc <- round(mean(dt_train_predictions == train_data$CLUSTER), 2)
dt_test_acc <- round(mean(dt_test_predictions == test_data$CLUSTER), 2)

# Create comparison dataframe
dt_acc_comp <- data.frame(
  Set = c("Train", "Test"),
  Accuracy = c(dt_train_acc, dt_test_acc))

# Display the formatted table with kable
dt_acc_comp %>%
  kable(caption = "Decision Tree Accuracy Comparison (Train vs Test)", 
        col.names = c("Dataset", "Accuracy")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The model has an accuracy of 91% on both the train and test sets, demonstrating strong performance across all clusters. For **Cluster 1: Less Active Customers with Low Order Volume**, precision is 88.6% and recall is 99.3%. In **Cluster 2: High Demand Customers**, precision is 89.5% and recall is 95.9%. For **Cluster 3: Intermediate Customers with Growth Potential**, precision is 92.5% and recall is 88.7%.

Overall, the model performs well across all clusters, with high recall and precision in **Cluster 1** and **Cluster 3**, and solid performance in **Cluster 2**. The accuracy comparison between the train and test sets is identical at 91%, indicating good generalization.

```{r}
# Extract and display variable importance from the trained decision tree model
var_importance <- decision_tree_model$finalModel$variable.importance
dt_var_importance_df <- data.frame(
  Variable = names(var_importance),
  Importance = var_importance
)

# Normalize importance values
dt_var_importance_df <- dt_var_importance_df %>%
  mutate(Importance = Importance / max(Importance))

# Sort by importance and visualize top 10 variables
dt_var_importance_df <- dt_var_importance_df %>% 
  arrange(desc(Importance)) %>%
  head(10)

# Plot the top 10 most important variables
ggplot(dt_var_importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "seagreen") +
  coord_flip() +
  labs(
    title = "Top 10 Variables Explaining Customer Clusters",
    subtitle = "Decision Tree Variable Importance",
    x = NULL,
    y = "Relative Importance (Normalized)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    axis.title = element_text(face = "bold"),
    panel.grid.major.y = element_blank()
  )

```

The most important variables in the model were the number of orders per customer, average days between orders, RFM score, total ordered volume, total cost, and the low demand customers flag.


### 10.2 Multinomial Logistic Regression

We will explore how the selected variables influence the customer clusters using multinomial logistic regression, in order to predict the probabilities of new customers belonging to each of the established clusters. This method is ideal for modeling the relationship between the predictors and the probabilities of customers being assigned to one of the three clusters, helping to assess the likelihood of a customer belonging to each specific group based on their characteristics.

We will use variable standardization and Elastic Net regularization for model development.

```{r, warning=FALSE}
# Normalize predictors
preprocess_params <- preProcess(model_data, method = c("center", "scale"))
model_data <- predict(preprocess_params, model_data)

# Create train/test split (70% train, 30% test)
set.seed(500)  
train_indices <- createDataPartition(model_data$CLUSTER, p = 0.7, list = FALSE)
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Set up cross-validation 
train_control <- trainControl(method = "cv", number = 10)

# Define a smaller tuning grid for efficiency
tune_grid <- expand.grid(alpha = 0.5, 
                         lambda = seq(0.1, 1, length = 5))

# Train model with Elastic Net regularization
mlogistic_model <- train(
  CLUSTER ~ ., 
  data = train_data,  
  method = "glmnet",
  trControl = train_control,
  tuneGrid = tune_grid,
  control = list(maxit = 200000),
)

# Print trained model summary
print(mlogistic_model)

# Make predictions on the test set
mlogis_predictions <- predict(mlogistic_model, test_data)

# Evaluate model performance
mlogistic_model_performance <- postResample(pred = mlogis_predictions, obs = test_data$CLUSTER)
print(mlogistic_model_performance)

# Display confusion matrix
mlogistic_confusion_matrix <- confusionMatrix(mlogis_predictions, test_data$CLUSTER)
print(mlogistic_confusion_matrix)

```

```{r}
# Generate predictions on train set
train_predictions <- predict(mlogistic_model, train_data)

# Evaluate accuracy on train and test sets
mlogistic_train_acc <- round(postResample(pred = train_predictions, obs = train_data$CLUSTER)["Accuracy"], 2)
mlogistic_test_acc <- round(postResample(pred = mlogis_predictions, obs = test_data$CLUSTER)["Accuracy"], 2)

# Create comparison dataframe
mlogistic_acc_comp <- data.frame(
  Set = c("Train", "Test"),
  Accuracy = c(mlogistic_train_acc, mlogistic_test_acc)
)

# Display the formatted table
mlogistic_acc_comp %>%
  kable(caption = "Multinomial Logistic Regression Accuracy Comparison (Train vs Test)", 
        col.names = c("Dataset", "Accuracy")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

The model achieved an accuracy of 90% on the train set and 89% on the test set, demonstrating good performance. In Cluster 1: Low Activity Customers, recall is 90.6% and precision is 91.8%. For Cluster 2: Medium Demand Customers, recall is 69.3% and precision is 99.6%. Finally, Cluster 3: High Demand Customers shows recall of 95.8% and precision of 84.9%. Overall, the model performs well, with higher recall in Cluster 3 and strong precision in Cluster 2.

The low recall in Cluster 2: Medium Demand Customers (69.3%) indicates that the model struggles to correctly identify customers in this group, leading to false negatives. 


```{r, warning=FALSE}
# Extract variable importance from the multinomial model
variable_importance <- varImp(mlogistic_model, scale = TRUE)

# Define custom colors for the clusters
palette_clusters <- c(
  "1" = "#FF6347",  # Coral
  "2" = "#4682B4",  # Cornflower blue
  "3" = "#FFD700")  # Yellow

# Extract importance data
var_imp_df <- as.data.frame(variable_importance$importance)
var_imp_df$Variable <- rownames(var_imp_df)

# Make sure the reshape2 package is loaded for melt function
library(reshape2)

# Convert to long format
var_imp_long <- melt(var_imp_df, 
                     id.vars = "Variable", 
                     variable.name = "Cluster", 
                     value.name = "Importance")

# Clean up cluster names (remove 'Overall' if present)
var_imp_long$Cluster <- gsub("Overall", "", var_imp_long$Cluster)

# Keep only top 10 variables per cluster for better visualization
top_vars <- var_imp_long %>%
  group_by(Cluster) %>%
  top_n(10, Importance) %>%
  ungroup()

# Create visualization with custom cluster colors
# Create visualization with custom cluster colors and no color legend
ggplot(var_imp_long, aes(x = reorder(Variable, Importance), y = Importance, fill = Cluster)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  facet_wrap(~ Cluster, scales = "free_x") +
  scale_fill_manual(values = palette_clusters) +  # Apply custom colors
  # Set the y-axis (importance) to have the same scale 0-100 for all facets
  scale_y_continuous(limits = c(0, 100)) +
  labs(
    title = "Multinomial Logistic Regression",
    subtitle = "Variable Importance by Cluster",
    x = "Variables",
    y = "Importance"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray30"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.title = element_text(face = "bold"),
    axis.text.y = element_text(size = 9),
    panel.grid.major.y = element_blank()
  ) +
  guides(fill = "none")  # Remove the color legend
```

The model indicates that for **Cluster 1**, the most important variables were the average number of days between orders, low demand customers, RFM score, and days since the first delivery.

For **Cluster 2**, the key variables included the number of orders, RFM score, order type (MyCoke Legacy), order type (MyCoke 360), order type (using sales representatives), and chain member.

For **Cluster 3**, the most significant variables were the average number of days between orders, low demand customers, order type (call center), order type (MyCoke Legacy), and channel growth potential.


The models created to predict clusters for new customers performed well and provide insights that clearly help in understanding the characteristics influencing the clusters. Therefore, we can proceed with the final analysis for fleet assignment.


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# List all variables in the environment
all_vars <- ls()

# Exclude 'full_data', 'full_data_customer', and the new variables from removal
vars_to_keep <- c("full_data", "full_data_customer","mydir", "one_seed", "reference_date")

# Get the variables to remove
vars_to_remove <- setdiff(all_vars, vars_to_keep)

# Remove the temporary data frames
rm(list = vars_to_remove)

# Clean up by removing 'all_vars' and 'vars_to_remove'
rm(all_vars, vars_to_remove)
```

